---
title: "model_fit"
output: html_document
---

load libraries and clean data

```{r, results='hide'}
library(readr)
library(tidyverse)
library(boot)
library(glmnet)
library(caret)
library(pROC)
library(MASS)
library(ggplot2)

set.seed(445) 

bank_full <- read_delim("data/bank-full_1.csv", 
    delim = ";", escape_double = FALSE, trim_ws = TRUE)
```


```{r}
bank_full$y <- factor(bank_full$y, levels = c("no", "yes"))


bank_full <- bank_full %>%
  mutate(
    job = as.factor(job),
    marital = as.factor(marital),
    education = as.factor(education),
    default = as.factor(default),
    housing = as.factor(housing),
    loan = as.factor(loan),
    contact = as.factor(contact),
    month = as.factor(month),
    poutcome = as.factor(poutcome)) %>% 
  mutate(previously_contacted = as.factor(ifelse(pdays == -1, "no", "yes")))

```


This initial phase is a crucial part of the modeling pipeline. This includes data cleaning, and the training/test split. The primary purpose of the training/test split is to perform an honest out-of-sample evaluation, a process where a model is fit exclusively on the training data and its performance is later assessed on the unseen test data to provide an estimate of the test error. Converting categorical variables to factors is necessary for R's statistical modeling functions, such as glm, to correctly interpret them as nominal predictors and create the appropriate dummy variables for inclusion in the model. The new previously_contacted variable also handles a common issue in this data set, where a value of -1 for pdays indicates the client was not previously contacted, effectively creating a binary feature.

## Train/Test split
To estimate the test error rate, the data is partitioned into a training set (80%) and a test set (20%). The model is fit only on the training data, and its performance is later evaluated on the unseen test data. The createDataPartition function is used to ensure a balanced split (stratified sampling) with respect to the response variable $Y$.

```{r}
train_index <- createDataPartition(bank_full$y, p = 0.8, list = FALSE, times = 1)
train_data <- bank_full[train_index, ]
test_data  <- bank_full[-train_index, ]

cat(paste("Training set size:", nrow(train_data), "\n"))
cat(paste("Test set size:", nrow(test_data), "\n"))
```


# 2. Model Selection: Backward Step-wise Selection
Model selection is employed to identify a parsimonious subset of predictors that minimizes the estimated test error. We use backward step-wise selection (Chapter 6), which starts with a full model containing all predictors and iteratively removes the variable that leads to the largest decrease (or smallest increase) in AIC (Akaike Information Criterion). AIC is an estimate of the test error, penalizing models with more predictors to favor simplicity.

### Initial Feature Selection
We first fit a full model, and then use the step() function to perform the selection based on AIC.


```{r}
full_formula <- y ~ age + job + marital + education + default + balance + 
  housing + loan + contact + day + month + duration + campaign + 
  pdays + previous + poutcome + previously_contacted

glm.full <- glm(full_formula, data = train_data, family = "binomial")

# Perform backward selection
glm.step <- step(glm.full, direction = "backward", trace = 0)


selected_formula <- formula(glm.step)
selected_formula
```


The aim of this process is to reduce the variance of the coefficient estimates by eliminating predictors that contribute little to the model's fit. The resulting selected_formula retains the most important predictors (job, housing, poutcome, duration, campaign) while eliminating others (age, default, pdays). Variables like duration (the time of the last contact) are known to be strong predictors, while pdays was made redundant by the engineered previously_contacted feature. The selected model aims for a better trade-off between bias (which may increase slightly by removing variables) and variance (which decreases, leading to a potentially lower overall test error).





# 3. Baseline Logistic Regression Model
We now fit the baseline model using the features identified through backward selection. Logistic regression (Chapter 4) models the probability of the response $Y=1$ (a 'yes' subscription) as:$$P(Y=1 | X) = \frac{e^{\beta_0 + \beta_1 X_1 + \dots + \beta_p X_p}}{1 + e^{\beta_0 + \beta_1 X_1 + \dots + \beta_p X_p}}$$The coefficients $\beta_j$ are interpreted as the change in the log-odds of a positive response associated with a one-unit increase in the predictor $X_j$.

```{r}
glm.baseline <- glm(selected_formula, data = train_data, family = "binomial")
summary(glm.baseline)
```

Assessing Non-Linearity with Residuals
A fundamental assumption in logistic regression is the linearity in the log-odds. We check this assumption for the continuous predictors by plotting the deviance residuals against the predictor values. Systematic deviation of the LOESS (locally estimated scatterplot smoothing) curve from zero suggests a need for a non-linear transformation or a more flexible model.


```{r}
# Extract deviance residuals
residuals <- residuals(glm.baseline, type = "deviance")
train_data$residuals <- residuals


p1 <- ggplot(train_data, aes(x = balance, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "blue") +
  labs(title = "Deviance Residuals vs. Balance",
       y = "Deviance Residuals") +
  theme_minimal()
p1
```

The plot of the deviance residuals against balance shows that the LOESS curve deviates non-trivially from the zero line. This non-flat pattern suggests that the relationship between balance and the log-odds is non-linear, violating a core assumption of the simple linear logistic regression. This systematic error (or high bias) indicates that the model's fit could be substantially improved by incorporating a non-linear transformation of balance, which would better capture the true underlying relationship.


```{r}
glm.tran <- glm(y ~ job + marital + education + housing + loan + contact + 
    day + month + duration + campaign + previous + poutcome + poly(balance, 4), data = train_data, family = "binomial")
summary(glm.tran)


residuals <- residuals(glm.tran, type = "deviance")
train_data$residuals <- residuals

ggplot(train_data, aes(x = balance, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "blue") +
  labs(title = "Deviance Residuals vs. Balance",
       y = "Deviance Residuals") +
  theme_minimal()
```


### Cross-Validation 
for Test Error EstimationTo obtain a reliable, low-variance estimate of the test error, we employ $K=5$ cross-validation (Chapter 5, Section 5.1.3) using the cv.glm() function. The cross-validated classification error rate is often used as a direct estimator of the true test error.

```{r}
cost <- function(r, pi) {
  mean(r != (pi > 0.5))
}

cv.error.baseline <- cv.glm(
  data = train_data, 
  glmfit = glm.tran, 
  cost = cost, 
  K = 5
)

cv_error_rate <- cv.error.baseline$delta[2]
round(cv_error_rate, 5)
```

The cross-validation process provides an estimate of the test error without requiring us to use the final, pristine test set. The resulting $\mathbf{K}$-fold CV error rate of round(cv_error_rate, 5) serves as a key metric for model selection. This value represents the expected proportion of observations that would be misclassified by the baseline model if deployed on new data.


# 4. Regularized Logistic Regression
To further reduce the model's variance and potentially find a model with a lower test error—especially in the presence of many correlated predictors—we apply regularization (Chapter 6). We use the glmnet package, which requires a design matrix for predictors ($X$) and a vector for the response ($Y$).

```{r}
X_full <- model.matrix(y ~ . - 1, data = bank_full)
Y_full <- as.numeric(bank_full$y) - 1 


X_train <- X_full[train_index, ]
Y_train <- Y_full[train_index] 

X_test <- X_full[-train_index, ]
Y_test <- Y_full[-train_index] 
```


### Ridge Regression ($\ell_2$ Penalty)
Ridge regression uses an $\ell_2$ penalty ($\alpha=0$) to shrink the coefficient estimates towards zero. This is a form of shrinkage that helps with multicollinearity and reduces variance, but it does not perform feature selection (no coefficients are set exactly to zero). The optimal tuning parameter $\lambda$ is selected via cross-validation.

```{r}
cv.ridge <- cv.glmnet(X_train, Y_train, 
                      alpha = 0, 
                      family = "binomial", 
                      type.measure = "class")
lambda.min.ridge <- cv.ridge$lambda.min

plot(cv.ridge)
```

The plot shows the cross-validated misclassification error (the measure of performance) as a function of $\log(\lambda)$. The $\lambda_{\min}$ chosen, which minimizes this cross-validated error, is r round(lambda.min.ridge, 5). This value represents the optimal level of shrinkage required to minimize the estimated test error. Note that Ridge regression does not set coefficients exactly to zero, meaning it retains all predictors but reduces the magnitude of their estimates.


### LASSO Regression ($\ell_1$ Penalty)
LASSO (Least Absolute Shrinkage and Selection Operator) regression uses an $\ell_1$ penalty ($\alpha=1$). A key property of the LASSO is that it forces the coefficients of some variables to be exactly zero, thereby performing automatic feature selection (Chapter 6).

```{r}
cv.lasso <- cv.glmnet(X_train, Y_train, 
                      alpha = 1, 
                      family = "binomial", 
                      type.measure = "class")
lambda.min.lasso <- cv.lasso$lambda.min

plot(cv.lasso)
```

The LASSO plot similarly uses cross-validation to select the optimal $\lambda_{\min}$, r round(lambda.min.lasso, 5). Unlike Ridge, the LASSO is expected to yield a more parsimonious model by excluding irrelevant predictors entirely. 



# 5. Model Assessment and Comparison
The ultimate goal is to assess the performance of the three models (Baseline, Ridge, and LASSO) on the unseen test set. We focus on the classification error rate (or accuracy) and, crucially for this imbalanced dataset, the Sensitivity and Specificity (Chapter 4).

### Test Set Predictions

```{r}
baseline_prob <- predict(glm.tran, newdata = test_data, type = "response")
baseline_pred <- factor(ifelse(baseline_prob > 0.5, "yes", "no"), levels = c("no", "yes"))

ridge_prob <- predict(cv.ridge, s = lambda.min.ridge, newx = X_test, type = "response")
ridge_pred <- factor(ifelse(ridge_prob > 0.5, "yes", "no"), levels = c("no", "yes"))

lasso_prob <- predict(cv.lasso, s = lambda.min.lasso, newx = X_test, type = "response")
lasso_pred <- factor(ifelse(lasso_prob > 0.5, "yes", "no"), levels = c("no", "yes"))
```


### Confusion Matrices and Metrics
The confusion matrix (Chapter 4) provides a detailed breakdown of the model's performance by counting True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).

```{r}
cm_baseline <- confusionMatrix(baseline_pred, test_data$y, positive = "yes")
cm_ridge <- confusionMatrix(ridge_pred, test_data$y, positive = "yes")
cm_lasso <- confusionMatrix(lasso_pred, test_data$y, positive = "yes")

metrics <- data.frame(
  Model = c("Baseline Logit", "Ridge Logit", "LASSO Logit"),
  Accuracy = c(cm_baseline$overall['Accuracy'], cm_ridge$overall['Accuracy'], cm_lasso$overall['Accuracy']),
  Sensitivity = c(cm_baseline$byClass['Sensitivity'], cm_ridge$byClass['Sensitivity'], cm_lasso$byClass['Sensitivity']),
  Specificity = c(cm_baseline$byClass['Specificity'], cm_ridge$byClass['Specificity'], cm_lasso$byClass['Specificity'])
)
metrics
```

All three models exhibit a high overall Accuracy (around 0.89), which is expected given the significant class imbalance (Prevalence $\approx 0.11$). However, the critical metric, Sensitivity (the True Positive Rate for the minority class, 'yes'), is very low (around 0.17). This result, consistent across all three models, indicates that they struggle to correctly identify subscribers, leading to many False Negatives (FN). Conversely, the high Specificity (True Negative Rate $\approx 0.98$) means the models are excellent at identifying non-subscribers. The regularization methods (Ridge and LASSO) failed to provide any significant improvement in these metrics, suggesting that the primary limitation is one of high bias (due to insufficient flexibility) rather than high variance.


### ROC Curve and AUC Comparison
The Receiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) (Chapter 4, Section 4.4) are threshold-independent measures of model discrimination. The AUC represents the probability that the model will rank a randomly chosen positive observation higher than a randomly chosen negative observation.

```{r}
roc_baseline <- roc(test_data$y, baseline_prob)
roc_ridge <- roc(test_data$y, as.numeric(ridge_prob))
roc_lasso <- roc(test_data$y, as.numeric(lasso_prob))

roc_data <- data.frame(
  fpr = c(1 - roc_baseline$specificities, 1 - roc_ridge$specificities, 1 - roc_lasso$specificities),
  tpr = c(roc_baseline$sensitivities, roc_ridge$sensitivities, roc_lasso$sensitivities),
  Model = factor(c(rep("Baseline Logit", length(roc_baseline$specificities)),
                   rep("Ridge Logit", length(roc_ridge$specificities)),
                   rep("LASSO Logit", length(roc_lasso$specificities))),
                 levels = c("Baseline Logit", "Ridge Logit", "LASSO Logit"))
)

ggplot(roc_data, aes(x = fpr, y = tpr, color = Model)) +
  geom_line(linewidth = 1) +
  geom_abline(linetype = "dashed", color = "grey") +
  labs(
    title = "ROC Curve Comparison for Logistic Models",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  theme_minimal()

auc_values <- data.frame(
  Model = c("Baseline Logit", "Ridge Logit", "LASSO Logit"),
  AUC = c(auc(roc_baseline), auc(roc_ridge), auc(roc_lasso))
)
auc_values
```

All three models yield a very similar AUC of approximately $0.90$. The ROC curves are virtually identical, demonstrating that both the Ridge and LASSO regularization penalties provided negligible improvement in the overall power of the model to distinguish between the two classes when compared to the simpler Baseline logistic regression. This consistent performance confirms the observation from the confusion matrices: while the logistic models are good at discrimination (AUC $> 0.5$), they lack the flexibility needed to substantially reduce the bias and improve the classification of the highly valuable minority class.


---




## 3.4 K-Nearest Neighbors (KNN)
- Standardize numeric predictors.
- Perform CV to tune **k** .
- Report best k and corresponding CV error.
- Discuss bias-variance behavior.

## 3.5 Generalized Additive Models (GAM)
- Fit GAM with smoothing splines for continuous variables.
- Use CV to choose smoothing parameters.
- Examine partial effect plots for interpretability.

## 3.6 Boosting (Most Flexible Model)
- Fit gradient boosting classifier (e.g., `gbm` or `xgboost`).
- Use cross-validation for:
  - number of trees,
  - interaction depth,
  - learning rate.
- Discuss complexity vs interpretability tradeoff. This model will be our benchmark non-interpretable model to compare interpretable models fitted above.

# 4. Results — Model Assessment & Comparison  

## 4.1 Test Set Performance
For each model:
- Confusion matrix  
  - Accuracy and misclassification rate  
  - Sensitivity / specificity (important for minority class)  
- ROC curves and AUC  

## 4.2 Comparative Evaluation
- Table summarizing all test-set performance metrics.
- Visual comparison of ROC curves.
- Discuss interpretability vs predictive quality.





