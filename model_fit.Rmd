---
title: "model_fit"
output: html_document
---

load libraries and clean data

```{r, results='hide'}
library(readr)
library(tidyverse)
library(boot)
library(glmnet)
library(caret)
library(pROC)
library(MASS)
library(ggplot2)
library(dplyr)

set.seed(445) 

bank_full <- read_delim("data/bank-full_1.csv", 
    delim = ";", escape_double = FALSE, trim_ws = TRUE)
```


```{r}
bank_full$y <- factor(bank_full$y, levels = c("no", "yes"))


bank_full <- bank_full %>%
  mutate(
    job = as.factor(job),
    marital = as.factor(marital),
    education = as.factor(education),
    default = as.factor(default),
    housing = as.factor(housing),
    loan = as.factor(loan),
    contact = as.factor(contact),
    month = as.factor(month),
    poutcome = as.factor(poutcome)) %>% 
  mutate(previously_contacted = as.factor(ifelse(pdays == -1, "no", "yes")))

```


This initial phase is a crucial part of the modeling pipeline. This includes data cleaning, and the training/test split. The primary purpose of the training/test split is to perform an honest out-of-sample evaluation, a process where a model is fit exclusively on the training data and its performance is later assessed on the unseen test data to provide an estimate of the test error. Converting categorical variables to factors is necessary for R's statistical modeling functions, such as glm, to correctly interpret them as nominal predictors and create the appropriate dummy variables for inclusion in the model. The new previously_contacted variable also handles a common issue in this data set, where a value of -1 for pdays indicates the client was not previously contacted, effectively creating a binary feature.

## Train/Test split
To estimate the test error rate, the data is partitioned into a training set (80%) and a test set (20%). The model is fit only on the training data, and its performance is later evaluated on the unseen test data. The createDataPartition function is used to ensure a balanced split (stratified sampling) with respect to the response variable $Y$.

```{r}
train_index <- createDataPartition(bank_full$y, p = 0.8, list = FALSE, times = 1)
train_data <- bank_full[train_index, ]
test_data  <- bank_full[-train_index, ]

cat(paste("Training set size:", nrow(train_data), "\n"))
cat(paste("Test set size:", nrow(test_data), "\n"))
```


# 2. Model Selection: Backward Step-wise Selection
Model selection is employed to identify a parsimonious subset of predictors that minimizes the estimated test error. We use backward step-wise selection (Chapter 6), which starts with a full model containing all predictors and iteratively removes the variable that leads to the largest decrease (or smallest increase) in AIC (Akaike Information Criterion). AIC is an estimate of the test error, penalizing models with more predictors to favor simplicity.

### Initial Feature Selection
We first fit a full model, and then use the step() function to perform the selection based on AIC.


```{r}
full_formula <- y ~ age + job + marital + education + default + balance + 
  housing + loan + contact + day + month + duration + campaign + 
  pdays + previous + poutcome + previously_contacted

glm.full <- glm(full_formula, data = train_data, family = "binomial")

# Perform backward selection
glm.step <- step(glm.full, direction = "backward", trace = 0)


selected_formula <- formula(glm.step)
selected_formula
```


The aim of this process is to reduce the variance of the coefficient estimates by eliminating predictors that contribute little to the model's fit. The resulting selected_formula retains the most important predictors (job, housing, poutcome, duration, campaign) while eliminating others (age, default, pdays). Variables like duration (the time of the last contact) are known to be strong predictors, while pdays was made redundant by the engineered previously_contacted feature. The selected model aims for a better trade-off between bias (which may increase slightly by removing variables) and variance (which decreases, leading to a potentially lower overall test error).





# 3. Baseline Logistic Regression Model
We now fit the baseline model using the features identified through backward selection. Logistic regression (Chapter 4) models the probability of the response $Y=1$ (a 'yes' subscription) as:$$P(Y=1 | X) = \frac{e^{\beta_0 + \beta_1 X_1 + \dots + \beta_p X_p}}{1 + e^{\beta_0 + \beta_1 X_1 + \dots + \beta_p X_p}}$$The coefficients $\beta_j$ are interpreted as the change in the log-odds of a positive response associated with a one-unit increase in the predictor $X_j$.

```{r}
glm.baseline <- glm(selected_formula, data = train_data, family = "binomial")
summary(glm.baseline)
```


```{r}
ctrl <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE, 
  summaryFunction = twoClassSummary 
)

set.seed(445)
cv_fit <- train(
  selected_formula, 
  data = train_data, 
  method = "glm", 
  family = "binomial",
  trControl = ctrl, 
  metric = "ROC" 
)

print(cv_fit)


test_probabilities <- predict(
  glm.baseline, 
  newdata = test_data, 
  type = "response"
)


test_predictions <- factor(
  ifelse(test_probabilities > 0.5, "yes", "no"),
  levels = levels(test_data$y)
)

conf_matrix <- confusionMatrix(
  data = test_predictions, 
  reference = test_data$y, 
  positive = "yes" 
)

print(conf_matrix)

```

Assessing Non-Linearity with Residuals
A fundamental assumption in logistic regression is the linearity in the log-odds. We check this assumption for the continuous predictors by plotting the deviance residuals against the predictor values. Systematic deviation of the LOESS (locally estimated scatterplot smoothing) curve from zero suggests a need for a non-linear transformation or a more flexible model.


```{r}
# Extract deviance residuals
residuals <- residuals(glm.baseline, type = "deviance")
train_data$residuals <- residuals


p1 <- ggplot(train_data, aes(x = balance, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "blue") +
  labs(title = "Deviance Residuals vs. Balance",
       y = "Deviance Residuals") +
  theme_minimal()
p1
```

The plot of the deviance residuals against balance shows that the LOESS curve deviates non-trivially from the zero line. This non-flat pattern suggests that the relationship between balance and the log-odds is non-linear, violating a core assumption of the simple linear logistic regression. This systematic error (or high bias) indicates that the model's fit could be substantially improved by incorporating a non-linear transformation of balance, which would better capture the true underlying relationship.


```{r}
glm.tran <- glm(y ~ job + marital + education + housing + loan + contact + 
    day + month + duration + campaign + previous + poutcome + poly(balance, 4), data = train_data, family = "binomial")
summary(glm.tran)
tran.coef <- summary(glm.tran)$coef


residuals <- residuals(glm.tran, type = "deviance")
train_data$residuals <- residuals

p2 <- ggplot(train_data, aes(x = balance, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "blue") +
  labs(title = "Deviance Residuals vs. Balance",
       y = "Deviance Residuals") +
  theme_minimal()
```


### Cross-Validation 
for Test Error EstimationTo obtain a reliable, low-variance estimate of the test error, we employ $K=5$ cross-validation (Chapter 5, Section 5.1.3) using the cv.glm() function. The cross-validated classification error rate is often used as a direct estimator of the true test error.

```{r}
cost <- function(r, pi) {
  mean(r != (pi > 0.5))
}

cv.error.baseline <- cv.glm(
  data = train_data, 
  glmfit = glm.tran, 
  cost = cost, 
  K = 5
)

cv_error_rate <- cv.error.baseline$delta[2]
round(cv_error_rate, 5)
```

The cross-validation process provides an estimate of the test error without requiring us to use the final, pristine test set. The resulting $\mathbf{K}$-fold CV error rate of round(cv_error_rate, 5) serves as a key metric for model selection. This value represents the expected proportion of observations that would be misclassified by the baseline model if deployed on new data.


# 4. Regularized Logistic Regression
To further reduce the model's variance and potentially find a model with a lower test error—especially in the presence of many correlated predictors—we apply regularization (Chapter 6). We use the glmnet package, which requires a design matrix for predictors ($X$) and a vector for the response ($Y$).

```{r}
X_full <- model.matrix(y ~ . - 1, data = bank_full)
Y_full <- as.numeric(bank_full$y) - 1 


X_train <- X_full[train_index, ]
Y_train <- Y_full[train_index] 

X_test <- X_full[-train_index, ]
Y_test <- Y_full[-train_index] 
```


### Ridge Regression ($\ell_2$ Penalty)
Ridge regression uses an $\ell_2$ penalty ($\alpha=0$) to shrink the coefficient estimates towards zero. This is a form of shrinkage that helps with multicollinearity and reduces variance, but it does not perform feature selection (no coefficients are set exactly to zero). The optimal tuning parameter $\lambda$ is selected via cross-validation.

```{r}
cv.ridge <- cv.glmnet(X_train, Y_train, 
                      alpha = 0, 
                      family = "binomial", 
                      type.measure = "class")
lambda.min.ridge <- cv.ridge$lambda.min

plot(cv.ridge)
```

The plot shows the cross-validated misclassification error (the measure of performance) as a function of $\log(\lambda)$. The $\lambda_{\min}$ chosen, which minimizes this cross-validated error, is r round(lambda.min.ridge, 5). This value represents the optimal level of shrinkage required to minimize the estimated test error. Note that Ridge regression does not set coefficients exactly to zero, meaning it retains all predictors but reduces the magnitude of their estimates.


### LASSO Regression ($\ell_1$ Penalty)
LASSO (Least Absolute Shrinkage and Selection Operator) regression uses an $\ell_1$ penalty ($\alpha=1$). A key property of the LASSO is that it forces the coefficients of some variables to be exactly zero, thereby performing automatic feature selection (Chapter 6).

```{r}
cv.lasso <- cv.glmnet(X_train, Y_train, 
                      alpha = 1, 
                      family = "binomial", 
                      type.measure = "class")
lambda.min.lasso <- cv.lasso$lambda.min

plot(cv.lasso)
```

The LASSO plot similarly uses cross-validation to select the optimal $\lambda_{\min}$, `r round(lambda.min.lasso, 5)`. Unlike Ridge, the LASSO is expected to yield a more parsimonious model by excluding irrelevant predictors entirely. 



# 5. Model Assessment and Comparison
The ultimate goal is to assess the performance of the three models (Baseline, Ridge, and LASSO) on the unseen test set. We focus on the classification error rate (or accuracy) and, crucially for this imbalanced dataset, the Sensitivity and Specificity (Chapter 4).

### Test Set Predictions

```{r}
baseline_prob <- predict(glm.tran, newdata = test_data, type = "response")
baseline_pred <- factor(ifelse(baseline_prob > 0.5, "yes", "no"), levels = c("no", "yes"))

ridge_prob <- predict(cv.ridge, s = lambda.min.ridge, newx = X_test, type = "response")
ridge_pred <- factor(ifelse(ridge_prob > 0.5, "yes", "no"), levels = c("no", "yes"))

lasso_prob <- predict(cv.lasso, s = lambda.min.lasso, newx = X_test, type = "response")
lasso_pred <- factor(ifelse(lasso_prob > 0.5, "yes", "no"), levels = c("no", "yes"))
```


### Confusion Matrices and Metrics
The confusion matrix (Chapter 4) provides a detailed breakdown of the model's performance by counting True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).

```{r}
cm_baseline <- confusionMatrix(baseline_pred, test_data$y, positive = "yes")
cm_ridge <- confusionMatrix(ridge_pred, test_data$y, positive = "yes")
cm_lasso <- confusionMatrix(lasso_pred, test_data$y, positive = "yes")

metrics <- data.frame(
  Model = c("Baseline Logit", "Ridge Logit", "LASSO Logit"),
  Accuracy = c(cm_baseline$overall['Accuracy'], cm_ridge$overall['Accuracy'], cm_lasso$overall['Accuracy']),
  Sensitivity = c(cm_baseline$byClass['Sensitivity'], cm_ridge$byClass['Sensitivity'], cm_lasso$byClass['Sensitivity']),
  Specificity = c(cm_baseline$byClass['Specificity'], cm_ridge$byClass['Specificity'], cm_lasso$byClass['Specificity'])
)
metrics
```

All three models exhibit a high overall Accuracy (around 0.89), which is expected given the significant class imbalance (Prevalence $\approx 0.11$). However, the critical metric, Sensitivity (the True Positive Rate for the minority class, 'yes'), is very low (around 0.17). This result, consistent across all three models, indicates that they struggle to correctly identify subscribers, leading to many False Negatives (FN). Conversely, the high Specificity (True Negative Rate $\approx 0.98$) means the models are excellent at identifying non-subscribers. The regularization methods (Ridge and LASSO) failed to provide any significant improvement in these metrics, suggesting that the primary limitation is one of high bias (due to insufficient flexibility) rather than high variance.


### ROC Curve and AUC Comparison
The Receiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) (Chapter 4, Section 4.4) are threshold-independent measures of model discrimination. The AUC represents the probability that the model will rank a randomly chosen positive observation higher than a randomly chosen negative observation.

```{r}
roc_baseline <- roc(test_data$y, baseline_prob)
roc_ridge <- roc(test_data$y, as.numeric(ridge_prob))
roc_lasso <- roc(test_data$y, as.numeric(lasso_prob))

roc_data <- data.frame(
  fpr = c(1 - roc_baseline$specificities, 1 - roc_ridge$specificities, 1 - roc_lasso$specificities),
  tpr = c(roc_baseline$sensitivities, roc_ridge$sensitivities, roc_lasso$sensitivities),
  Model = factor(c(rep("Baseline Logit", length(roc_baseline$specificities)),
                   rep("Ridge Logit", length(roc_ridge$specificities)),
                   rep("LASSO Logit", length(roc_lasso$specificities))),
                 levels = c("Baseline Logit", "Ridge Logit", "LASSO Logit"))
)

ggplot(roc_data, aes(x = fpr, y = tpr, color = Model)) +
  geom_line(linewidth = 1) +
  geom_abline(linetype = "dashed", color = "grey") +
  labs(
    title = "ROC Curve Comparison for Logistic Models",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  theme_minimal()

auc_values <- data.frame(
  Model = c("Baseline Logit", "Ridge Logit", "LASSO Logit"),
  AUC = c(auc(roc_baseline), auc(roc_ridge), auc(roc_lasso))
)
auc_values
```

All three models yield a very similar AUC of approximately $0.90$. The ROC curves are virtually identical, demonstrating that both the Ridge and LASSO regularization penalties provided negligible improvement in the overall power of the model to distinguish between the two classes when compared to the simpler Baseline logistic regression. This consistent performance confirms the observation from the confusion matrices: while the logistic models are good at discrimination (AUC $> 0.5$), they lack the flexibility needed to substantially reduce the bias and improve the classification of the highly valuable minority class.


---

## 3.4 K-Nearest Neighbors (KNN)
KNN is a nonparametric method that relies on feature scaling and distance-based classification.

## Prepare scaled numeric predictors
```{r}
library(class)

numeric_vars <- names(train_data)[sapply(train_data, is.numeric)]
numeric_vars <- setdiff(numeric_vars, "residuals")

train_x <- train_data[, numeric_vars]
test_x  <- test_data[, numeric_vars]

train_x_scaled <- scale(train_x)
test_x_scaled <- scale(
  test_x,
  center = attr(train_x_scaled, "scaled:center"),
  scale  = attr(train_x_scaled, "scaled:scale")
)

train_y <- train_data$y
test_y  <- test_data$y

```

## Cross-validation to choose the best k

```{r}
k_values <- seq(1, 25, 2)
cv_accuracy <- numeric(length(k_values))

set.seed(445)
folds <- createFolds(train_y, k = 5)

for (i in seq_along(k_values)) {
  k <- k_values[i]
  fold_acc <- c()
  
  for (f in folds) {
    pred <- knn(
      train = train_x_scaled[-f, ],
      test  = train_x_scaled[f, ],
      cl    = train_y[-f],
      k = k
    )
    fold_acc <- c(fold_acc, mean(pred == train_y[f]))
  }
  
  cv_accuracy[i] <- mean(fold_acc)
}

best_k <- k_values[which.max(cv_accuracy)]
best_k

```

Using 5-fold cross-validation across odd values of kk from 1 to 25, the model achieved its highest average classification accuracy at k=23. This indicates that a relatively large number of neighbors provides the best bias–variance trade-off for this dataset. Because the Bank Marketing data is high-dimensional and noisy, smaller values of kk likely overfit the training data, while larger k values smooth out noise and generalize better. The fact that k=23 performs best suggests the underlying decision boundary is fairly smooth, and KNN benefits from stronger averaging across many neighbors.

## Final KNN prediction and metrics

```{r}
knn_pred <- knn(
  train = train_x_scaled,
  test  = test_x_scaled,
  cl    = train_y,
  k = best_k
)

cm_knn <- confusionMatrix(knn_pred, test_y, positive = "yes")
cm_knn

knn_prob <- as.numeric(knn_pred == "yes")
roc_knn <- roc(test_y, knn_prob)
auc_knn <- auc(roc_knn)
auc_knn

```

The KNN model with k=23 achieved an accuracy of 0.8911, which is comparable to the logistic regression models. However, due to the class imbalance, accuracy alone is misleading. The model’s sensitivity is very low (0.26), meaning it correctly identifies only about 26% of clients who actually subscribed. In contrast, the specificity is very high (0.97), showing the model is much better at identifying non-subscribers. The AUC of 0.617 confirms that KNN provides only limited discriminatory power for this dataset, performing slightly above random chance but worse than logistic regression and boosting models.


## 3.5 Generalized Additive Models (GAM)

GAM allows flexible nonlinear fits through smoothing splines.

```{r}
library(gam)

gam_model <- gam(
  y ~ s(age) + s(balance) + s(duration) + s(campaign) + s(previous),
  data = train_data,
  family = binomial
)

summary(gam_model)

gam_prob <- predict(gam_model, test_data, type = "response")
gam_pred <- ifelse(gam_prob > 0.5, "yes", "no") |> factor(levels = c("no","yes"))

cm_gam <- confusionMatrix(gam_pred, test_y, positive = "yes")
cm_gam

roc_gam <- roc(test_y, gam_prob)
auc_gam <- auc(roc_gam)
auc_gam

```

The GAM model allows nonlinear relationships through smoothing splines and the ANOVA results show that all five smoothed predictors—age, balance, duration, campaign, and previous—have statistically significant nonlinear effects on subscription probability (all p < 0.001). The model maintains an accuracy of 0.8888, similar to previous models, but like the other approaches, it struggles with the minority “yes” class. The sensitivity is 0.216, meaning the GAM correctly identifies only about 21% of subscribers, while specificity is very high at 0.978, reflecting accurate detection of non-subscribers. Despite the low sensitivity, the model achieves an AUC of 0.854, indicating strong overall discriminatory power. This suggests the GAM captures meaningful nonlinear patterns, but still struggles to translate these patterns into correct classifications for the rare positive cases.



## 3.6 Boosting (Most Flexible Model)

Boosting builds many small trees sequentially, improving mistakes made by prior trees.

```{r}

if ("residuals" %in% colnames(train_data)) {
  train_data$residuals <- NULL
}

train_data$y_num <- ifelse(train_data$y == "yes", 1, 0)
test_data$y_num  <- ifelse(test_data$y == "yes", 1, 0)

library(gbm)
set.seed(445)

boost_model <- gbm(
  y_num ~ . - y,         
  data = train_data,
  distribution = "bernoulli",
  n.trees = 1500,       
  shrinkage = 0.01,
  interaction.depth = 3,
  bag.fraction = 0.5,
  verbose = FALSE
)

best_trees <- gbm.perf(boost_model, method = "OOB", plot.it = FALSE)
best_trees

```

The boosting model was trained using 1500 trees, and the out-of-bag (OOB) performance evaluation selected all 1500 trees as optimal. This indicates that the model continued to improve throughout the entire boosting sequence, with no evidence of overfitting detected by the OOB error. The small residual standard error from the LOESS smoother (2.7 × 10⁻⁵) shows that the OOB improvement curve is smooth, stable, and consistently positive, suggesting reliable incremental improvement. The model's behavior reflects the strengths of boosting: its ability to gradually reduce bias and capture complex nonlinear and interaction effects in the data. The fact that performance continues to improve up to 1500 trees suggests that boosting may be the most flexible and predictive model among those considered.

Prediction + Metrics
```{r}
boost_prob <- predict(boost_model, test_data, n.trees = best_trees, type = "response")
boost_pred <- factor(ifelse(boost_prob > 0.5, "yes", "no"), levels = c("no","yes"))

cm_boost <- confusionMatrix(boost_pred, test_y, positive = "yes")
cm_boost

roc_boost <- roc(test_y, boost_prob)
auc_boost <- auc(roc_boost)
auc_boost

```
The boosting model achieved the strongest performance among all models evaluated. Its overall accuracy on the test set was 0.9047, exceeding both the baseline logistic regression and other flexible models such as KNN and GAM. Most importantly, boosting substantially improved sensitivity, correctly identifying 41.15% of actual subscribers (“yes”), compared to roughly 21–26% for the other models. This improvement indicates that boosting is better able to capture the complex, nonlinear structure of the minority class, which is critical in this imbalanced classification problem. Specificity remained high (96.99%), showing that the model continued to classify non-subscribers accurately.
Boosting also demonstrated stronger balance across classes, with a Balanced Accuracy of 0.6907, the highest among all models tested. Additionally, the model’s AUC value (insert your computed AUC here once available) reflects its enhanced ability to rank subscribers above non-subscribers. Together, these results reinforce that boosting provides the best predictive performance in terms of both overall classification and minority-class detection, aligning with expectations for a highly flexible, low-bias model.


# 4. Results — Model Assessment & Comparison  

## 4.1 Test Set Performance

```{r}
final_results <- data.frame(
  Model = c("Logistic", "Ridge", "LASSO", "KNN", "GAM", "Boosting"),
  Accuracy = c(
    cm_baseline$overall["Accuracy"],
    cm_ridge$overall["Accuracy"],
    cm_lasso$overall["Accuracy"],
    cm_knn$overall["Accuracy"],
    cm_gam$overall["Accuracy"],
    cm_boost$overall["Accuracy"]
  ),
  Sensitivity = c(
    cm_baseline$byClass["Sensitivity"],
    cm_ridge$byClass["Sensitivity"],
    cm_lasso$byClass["Sensitivity"],
    cm_knn$byClass["Sensitivity"],
    cm_gam$byClass["Sensitivity"],
    cm_boost$byClass["Sensitivity"]
  ),
  AUC = c(
    auc(roc_baseline),
    auc(roc_ridge),
    auc(roc_lasso),
    auc_knn,
    auc_gam,
    auc_boost
  )
)

final_results

```
The overall model comparison confirms that Boosting is the strongest predictive model in this project. It achieved the highest accuracy (0.9047), the highest sensitivity (0.4115), and the highest AUC (0.9266). These metrics show that boosting not only performs well overall but is also the most effective at identifying the minority “yes” class—an essential goal in the bank marketing problem.

The family of logistic models (Logistic, Ridge, and LASSO) all attained similar accuracy levels (≈0.898–0.900) and high AUC values (≈0.907), indicating solid discrimination ability. However, their sensitivity values (0.28–0.33) remain relatively low, meaning they miss many potential subscribers compared with boosting.

KNN performed worse overall, with lower accuracy (0.8911) and the lowest AUC (0.617), showing that it struggled with this high-dimensional, imbalanced dataset. GAM demonstrated moderate AUC (0.854), but its low sensitivity (0.2157) suggests it was unable to flexibly model the decision boundary needed to capture the minority class.

Overall, boosting delivered the most balanced and effective performance across all key metrics, supporting the conclusion that flexible ensemble methods are better suited for the nonlinear patterns present in the bank marketing data.

## 4.2 Comparative Evaluation
- Table summarizing all test-set performance metrics.
- Visual comparison of ROC curves.
- Discuss interpretability vs predictive quality.

Combined ROC Curve
```{r}
roc_all <- rbind(
  data.frame(fpr = 1 - roc_baseline$specificities, tpr = roc_baseline$sensitivities, Model = "Logistic"),
  data.frame(fpr = 1 - roc_ridge$specificities,    tpr = roc_ridge$sensitivities,    Model = "Ridge"),
  data.frame(fpr = 1 - roc_lasso$specificities,    tpr = roc_lasso$sensitivities,    Model = "LASSO"),
  data.frame(fpr = 1 - roc_knn$specificities,      tpr = roc_knn$sensitivities,      Model = "KNN"),
  data.frame(fpr = 1 - roc_gam$specificities,      tpr = roc_gam$sensitivities,      Model = "GAM"),
  data.frame(fpr = 1 - roc_boost$specificities,    tpr = roc_boost$sensitivities,    Model = "Boosting")
)

ggplot(roc_all, aes(x = fpr, y = tpr, color = Model)) +
  geom_line(size = 1) +
  geom_abline(linetype = "dashed") +
  labs(
    title = "ROC Curve Comparison for All Models",
    x = "False Positive Rate",
    y = "True Positive Rate"
  ) +
  theme_minimal()

```




