---
title: "model_fit"
output: html_document
---

load clean data

```{r}

```



## 2.3 Train/Test split
- Explain purpose of an honest out-of-sample evaluation.
- k-fold CV, k=5


---

# 3. Modeling 

## 3.1 Logistic Regression (Baseline Model)
- Fit logistic regression.
- k-fold CV for estimated test error.
- Assess linearity and consider interactions if appropriate.
- Optional: **regularized logistic regression** (Ridge/Lasso) using CV-selected $\lambda$.

## 3.2 Linear Discriminant Analysis (LDA)
- Fit LDA model.
- k-fold CV for estimated test error.
- Check assumptions (normality, equal covariance).
- Discuss bias-variance

## 3.3 Quadratic Discriminant Analysis (QDA)
- Fit QDA model.
- k-fold CV for estimated test error.
- Discuss overfitting risk with higher model flexibility.
- Compare performance with LDA.


## 3.4 K-Nearest Neighbors (KNN)
- Standardize numeric predictors.
- Perform CV to tune **k** .
- Report best k and corresponding CV error.
- Discuss bias-variance behavior.

## 3.5 Generalized Additive Models (GAM)
- Fit GAM with smoothing splines for continuous variables.
- Use CV to choose smoothing parameters.
- Examine partial effect plots for interpretability.

## 3.6 Boosting (Most Flexible Model)
- Fit gradient boosting classifier (e.g., `gbm` or `xgboost`).
- Use cross-validation for:
  - number of trees,
  - interaction depth,
  - learning rate.
- Discuss complexity vs interpretability tradeoff. This model will be our benchmark non-interpretable model to compare interpretable models fitted above.

---

# 4. Results â€” Model Assessment & Comparison  

## 4.1 Test Set Performance
For each model:
- Confusion matrix  
  - Accuracy and misclassification rate  
  - Sensitivity / specificity (important for minority class)  
- ROC curves and AUC  

## 4.2 Comparative Evaluation
- Table summarizing all test-set performance metrics.
- Visual comparison of ROC curves.
- Discuss interpretability vs predictive quality.





