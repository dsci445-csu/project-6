% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  ignorenonframetext,
]{beamer}
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{section title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{subsection title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\newif\ifbibliography
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
\usepackage{caption}
% Make caption package work with longtable
\makeatletter
\def\fnum@table{\tablename~\thetable}
\makeatother
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Statistical Machine Learning Models on A Bank Marketing Dataset},
  pdfauthor={Sandra Afrifa, Tigist Kefelew, Mussa Hassen, Waddah Alqahtani},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Statistical Machine Learning Models on A Bank Marketing Dataset}
\subtitle{Colorado State University, DSCI 445: Statistical Machine
Learning, Dr.Andee Kaplan}
\author{Sandra Afrifa, Tigist Kefelew, Mussa Hassen, Waddah Alqahtani}
\date{}

\begin{document}
\frame{\titlepage}

\begin{frame}{Project Motivation \& Business Problem}
\phantomsection\label{project-motivation-business-problem}
\begin{itemize}
\tightlist
\item
  Telemarketing campaigns are expensive, time-consuming, and have very
  low success rates. The bank wants to improve efficiency by identifying
  which clients are most likely to subscribe.
\item
  The dataset has a severe class imbalance: only a small fraction of
  customers subscribe to the term deposit. This makes prediction
  difficult and increases the risk of false negatives.
\item
  Improving predictive accuracy would allow the bank to target fewer
  customers, reduce costs, and increase return on investment (ROI) for
  future campaigns.
\end{itemize}
\end{frame}

\begin{frame}{The Bank Marketing Dataset}
\phantomsection\label{the-bank-marketing-dataset}
\begin{itemize}
\tightlist
\item
  \textbf{45,000 client observations} from Portuguese bank
  telemarketing\\
\item
  \textbf{Target variable (\(y\)):} Did client subscribe to term
  deposit? (Yes/No)\\
\item
  \textbf{Class imbalance:} Only ≈11\% ``Yes'' subscriptions\\
\item
  \textbf{Four main predictor groups:} demographics, financial
  indicators, campaign features, past outcomes
\end{itemize}
\end{frame}

\begin{frame}{Term Deposit Class Imbalance}
\phantomsection\label{term-deposit-class-imbalance}
\begin{itemize}
\tightlist
\item
  \textbf{Most clients say NO; only ≈11\% say YES}
\end{itemize}

\begin{center}\includegraphics[width=0.75\linewidth]{Slides_files/figure-beamer/imbalance-plot-1} \end{center}
\end{frame}

\begin{frame}{EDA Finding 1: Contact Method Matters}
\phantomsection\label{eda-finding-1-contact-method-matters}
\begin{itemize}
\tightlist
\item
  Cell phone calls perform best
\end{itemize}

\begin{center}\includegraphics[width=0.75\linewidth]{Slides_files/figure-beamer/contact-plot-1} \end{center}
\end{frame}

\begin{frame}{EDA Finding 2: Previous Success Predicts Future Success}
\phantomsection\label{eda-finding-2-previous-success-predicts-future-success}
\begin{itemize}
\tightlist
\item
  Past success predicts new success
\end{itemize}

\begin{center}\includegraphics[width=0.7\linewidth]{Slides_files/figure-beamer/poutcome-plot-1} \end{center}
\end{frame}

\begin{frame}{EDA Finding 3: Too Many Calls Hurts Results}
\phantomsection\label{eda-finding-3-too-many-calls-hurts-results}
\begin{itemize}
\tightlist
\item
  More calls = worse results
\end{itemize}

\begin{center}\includegraphics[width=0.75\linewidth]{Slides_files/figure-beamer/campaign-plot-1} \end{center}
\end{frame}

\begin{frame}{Methodology: Data Preparation}
\phantomsection\label{methodology-data-preparation}
\begin{itemize}
\tightlist
\item
  Converted all categorical variables into factors so models interpret
  them correctly.
\item
  Addressed ``unknown'' values by treating them as their own category to
  avoid losing information.
\item
  Created a new feature previously\_contacted to properly encode pdays.
\item
  Split the dataset into 80\% training and 20\% test using stratified
  sampling to preserve the class imbalance.
\item
  Ensured numeric predictors were available for later scaling, tuning,
  and model evaluation.
\end{itemize}
\end{frame}

\begin{frame}{Breakdown of Categorical Variable}
\phantomsection\label{breakdown-of-categorical-variable}
\pandocbounded{\includegraphics[keepaspectratio]{Slides_files/figure-beamer/unnamed-chunk-1-1.pdf}}
\end{frame}

\section{1. Logistic Regression Model}\label{logistic-regression-model}

\begin{frame}[fragile]{Feature Selection \& Linearity}
\phantomsection\label{feature-selection-linearity}
\begin{itemize}
\tightlist
\item
  Initial feature selection was performed using \textbf{Backward
  Stepwise Elimination} on the full model to find the most useful set of
  main effects.
\item
  Our selected formula resulted:
\end{itemize}

\(y \sim job + marital + education + balance + housing + loan + contact +\)

\(day + month + duration + campaign + previous+ poutcome\)

\begin{itemize}
\tightlist
\item
  Residual Plots were used to assess the linearity assumption for
  numeric predictors (\texttt{age}, \texttt{balance}). This led to
  testing a model with non-linear terms.
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\begin{verbatim}
## `geom_smooth()` using formula = 'y ~ x'
\end{verbatim}

\begin{center}\includegraphics{Slides_files/figure-beamer/residuals_slide-1} \end{center}
\end{frame}

\begin{frame}{Models with non-linear components and interactions}
\phantomsection\label{models-with-non-linear-components-and-interactions}
\begin{itemize}
\tightlist
\item
  A \textbf{quadratic term} \(I(\text{balance}^4)\)---was added to the
  formula to reduce the model's bias.
\item
  We then tested for crucial \textbf{interaction terms} (between
  campaign features and demographics).
\item
  Our model had a cross-validated test error rate 0.098`
\end{itemize}
\end{frame}

\begin{frame}{Regularization \& Tuning}
\phantomsection\label{regularization-tuning}
\begin{itemize}
\tightlist
\item
  We explored Ridge and LASSO regularization as an alternative approach
  for feature selection.
\item
  \textbf{5-Fold Cross-Validation} was used to select the optimal
  penalty parameter, \(\lambda\), which minimized the mean CV error for
  each regularized model.
\end{itemize}

\pandocbounded{\includegraphics[keepaspectratio]{Slides_files/figure-beamer/unnamed-chunk-2-1.pdf}}
\pandocbounded{\includegraphics[keepaspectratio]{Slides_files/figure-beamer/unnamed-chunk-2-2.pdf}}
\end{frame}

\begin{frame}{Evaluation}
\phantomsection\label{evaluation}
\begin{itemize}
\tightlist
\item
  The final selected LogReg model was evaluated on the 5-fold CV error
  rate, along with metrics focused on the confusion matrix.
\item
  Models performed relatively similar. We would prefer a model with
  higher \emph{sensitivity} (few false negatives)
\end{itemize}

\begin{longtable}[]{@{}lrrrr@{}}
\toprule\noalign{}
Model & Accuracy & Sensitivity & Specificity & AUC \\
\midrule\noalign{}
\endhead
Baseline Logit & 0.8996792 & 0.3301798 & 0.9750752 & 0.9077627 \\
Ridge Logit & 0.8982413 & 0.2800378 & 0.9800852 & 0.9080043 \\
LASSO Logit & 0.8981307 & 0.3008515 & 0.9772044 & 0.9072429 \\
\bottomrule\noalign{}
\end{longtable}
\end{frame}

\section{2. K-Nearest Neighbors (KNN)
Model}\label{k-nearest-neighbors-knn-model}

\begin{frame}{KNN: Feature Preparation}
\phantomsection\label{knn-feature-preparation}
\begin{itemize}
\tightlist
\item
  \textbf{KNN} requires scaling: all numeric predictors were
  \textbf{standardized} (mean=0, SD=1) to prevent features with larger
  scales from dominating the distance calculation.
\item
  The non-parametric nature means \textbf{no explicit feature selection}
  via coefficients, but the scaling step is vital for ensuring equal
  contribution from all dimensions.
\end{itemize}

\begin{center}\includegraphics{Slides_files/figure-beamer/knn-prep-1} \end{center}

\textbf{Test Accuracy (k=5):} 88.55\%
\end{frame}

\begin{frame}{KNN: Hyperparameter Tuning}
\phantomsection\label{knn-hyperparameter-tuning}
\begin{itemize}
\tightlist
\item
  The sole hyperparameter, \textbf{\(k\)} (the number of neighbors),
  determines the model's complexity (i.e., the bias-variance trade-off).
\item
  \textbf{5-Fold Cross-Validation} was performed across a range of \(k\)
  values to find the optimal \(k\) that minimized the mean CV error.
\end{itemize}

\begin{center}\includegraphics{Slides_files/figure-beamer/knn-tuning-1} \end{center}

\textbf{Optimal k:} 23
\end{frame}

\begin{frame}{KNN: Evaluation \& Test Performance}
\phantomsection\label{knn-evaluation-test-performance}
\begin{itemize}
\tightlist
\item
  The final KNN model (with optimal \(k\)) was applied to the
  \textbf{Test Set}. We evaluate performance using \textbf{AUC} and
  compare it to the LogReg models.
\item
  KNN generally provides a good intermediate balance---often capturing
  non-linearity better than LogReg but being less computationally
  expensive than boosting.
\end{itemize}

\begin{longtable}[]{@{}lrr@{}}
\toprule\noalign{}
Model & AUC & Sensitivity \\
\midrule\noalign{}
\endhead
KNN & 0.852 & 0.260 \\
Baseline Logit & 0.908 & 0.330 \\
Ridge & 0.908 & 0.280 \\
LASSO & 0.907 & 0.301 \\
\bottomrule\noalign{}
\end{longtable}
\end{frame}

\section{3. Boosting Tress Model}\label{boosting-tress-model}

\begin{frame}{Boosting: Model Formulation}
\phantomsection\label{boosting-model-formulation}
\begin{itemize}
\tightlist
\item
  Boosting builds many small trees sequentially, each fixing errors from
  the previous one.
\item
  These weak learners combine into a strong model that captures complex,
  non-linear patterns.
\item
  It usually achieves the highest accuracy/AUC, but is less
  interpretable than LogReg or KNN.
\end{itemize}
\end{frame}

\begin{frame}
\begin{center}\includegraphics[width=0.8\linewidth]{Slides_files/figure-beamer/boosting-viz-1} \end{center}
\end{frame}

\begin{frame}{Boosting: Hyperparameter Tuning}
\phantomsection\label{boosting-hyperparameter-tuning}
\begin{itemize}
\tightlist
\item
  We tuned the number of trees using OOB error estimation to prevent
  overfitting.
\item
  The OOB method selected 1100 trees as optimal.
\item
  This choice balances model complexity and generalization.
\end{itemize}
\end{frame}

\begin{frame}
\begin{center}\includegraphics[width=0.75\linewidth]{Slides_files/figure-beamer/boosting-tuning-1} \end{center}

\textbf{Optimal number of trees (via test error):} 1100
\end{frame}

\begin{frame}{Boosting: Evaluation \& Importance}
\phantomsection\label{boosting-evaluation-importance}
\begin{itemize}
\tightlist
\item
  The final Boosting model achieved the highest AUC and sensitivity
  among all models tested.
\item
  Variable Importance Plot shows which features influence predictions
  most.
\item
  The importance rankings align with key predictors identified by
  Logistic Regression.
\end{itemize}
\end{frame}

\begin{frame}
\pandocbounded{\includegraphics[keepaspectratio]{Slides_files/figure-beamer/boosting-results-1.pdf}}

\textbf{Boosting Test AUC:} 0.927\\
\textbf{Boosting Sensitivity:} 0.412\\
\textbf{Boosting Accuracy:} 0.905
\end{frame}

\begin{frame}{Results: Comparative Evaluation}
\phantomsection\label{results-comparative-evaluation}
\begin{itemize}
\tightlist
\item
  We evaluated five models using Accuracy, Sensitivity (Recall), and AUC
  on the Test Set.
\item
  \textbf{Boosting} achieved the highest overall accuracy and the
  highest sensitivity, meaning it identified the most true positives.
\item
  Logistic, Ridge, and LASSO models performed very similarly in accuracy
  and AUC, but with lower sensitivity.
\item
  KNN performed the worst, with the lowest AUC, indicating weak
  separation between classes.
\end{itemize}
\end{frame}

\begin{frame}{Model Performance Comparison}
\phantomsection\label{model-performance-comparison}
\begin{longtable}[]{@{}lrrr@{}}
\toprule\noalign{}
Model & Accuracy & Sensitivity & AUC \\
\midrule\noalign{}
\endhead
Logistic & 0.900 & 0.330 & 0.908 \\
Ridge & 0.898 & 0.280 & 0.908 \\
LASSO & 0.898 & 0.301 & 0.907 \\
KNN & 0.891 & 0.260 & 0.852 \\
Boosting & 0.905 & 0.412 & 0.927 \\
\bottomrule\noalign{}
\end{longtable}
\end{frame}

\begin{frame}{ROC Curve Comparison}
\phantomsection\label{roc-curve-comparison}
\begin{center}\includegraphics{Slides_files/figure-beamer/final-roc-1} \end{center}
\end{frame}

\begin{frame}{Business Impact}
\phantomsection\label{business-impact}
\begin{itemize}
\tightlist
\item
  The central trade-off: Boosting is the best predictor, but the
  \textbf{Regularized Logistic Regression} offers the most
  \emph{interpretable} and \emph{actionable} insights for the bank.
\item
  Sample coefficients: \(\beta_{Balance^2}\) = -23.02,
  \(\beta_{Housing}\) = -0.65, \(\beta_{JobStudent}\) = 4.064
\item
  The chosen model can lead to cost savings and higher ROI for future
  campaigns.
\end{itemize}
\end{frame}

\begin{frame}{Discussion}
\phantomsection\label{discussion}
\begin{itemize}
\tightlist
\item
  Consideration: regulatory/ethical implications in banking and the need
  to justify predictions (further supporting the value of
  interpretability).
\item
  Advanced train/test splitting methods like \emph{time-series analysis}
  were not used.
\item
  Suggest future improvements: Exploring advanced techniques like
  \emph{resampling methods} (SMOTE) to handle imbalance better or
  trying.
\end{itemize}
\end{frame}

\begin{frame}{References}
\phantomsection\label{references}
\begin{itemize}
\tightlist
\item
  UC Irvine Machine Learning Repository. Bank Marketing.
  \url{https://archive.ics.uci.edu/dataset/222/bank+marketing}
\item
  Github link: \url{https://github.com/dsci445-csu/project-6}
\end{itemize}

\begin{center}\includegraphics{UCI} \end{center}
\end{frame}

\end{document}
