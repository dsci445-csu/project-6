% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
  ignorenonframetext,
]{beamer}
\newif\ifbibliography
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% remove section numbering
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{section title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{subsection title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Statistical Machine Learning Models on A Bank Marketing dataset},
  pdfauthor={Sandra, Tigist, Mussa Hassen, Waddah},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Statistical Machine Learning Models on A Bank Marketing dataset}
\subtitle{Colorado State University, DSCI 445: Statistical Machine
Learning, Dr.Andee Kaplan}
\author{Sandra, Tigist, Mussa Hassen, Waddah}

\begin{document}
\frame{\titlepage}

\begin{frame}{Project Motivation \& Business Problem}
\protect\phantomsection\label{project-motivation-business-problem}
\begin{itemize}
\tightlist
\item
  \textbf{Discuss:} The bank's need for efficient marketing and the high
  cost of telemarketing campaigns. The goal is to maximize the success
  rate by predicting customer subscription.
\item
  \textbf{Discuss:} The challenge of less positive class (imbalanced
  data) and why prediction accuracy is crucial.
\item
  \textbf{Image/Plot Idea:} A simple, high-level info graphic of a
  marketing campaign funnel, highlighting the bottleneck/drop-off point.
\end{itemize}

{[}Image of a marketing funnel illustrating customer conversion{]}
\end{frame}

\begin{frame}{The Bank Marketing Dataset (Slide 2)}
\protect\phantomsection\label{the-bank-marketing-dataset-slide-2}
\begin{itemize}
\tightlist
\item
  \textbf{Discuss:} The dataset structure: \(\approx 45,000\) client
  observations from a Portuguese banking institution's telemarketing
  campaigns.
\item
  \textbf{Discuss:} Brief overview of the four main predictor
  categories: Client Demographics, Financial Indicators, Campaign
  Features, and Past Outcomes. Mention the paper had more features, fro
  privacy reasons the public data set has less
\item
  \textbf{Discuss:} The Target Variable (\(y\)) is binary: Did the
  client subscribe to a term deposit (Yes/No)?
\item
  \textbf{Discuss:} Emphasize the severity of the class imbalance (e.g.,
  typically \(\approx 11\%\) `Yes' subscriptions). This may yield higher
  false negatives.
\item
  \textbf{Image/Plot Idea:} A small table summarizing the list of the 4
  key variable groups (e.g., Demographics, Financial, Campaign, Past).
\item
  \textbf{Image/Plot Idea:} A \textbf{Bar Plot} showing the distribution
  of the target variable (\(y\)), visually highlighting the imbalance.
\end{itemize}
\end{frame}

\begin{frame}{Problem Statement \& Methodology Overview (Slide 3)}
\protect\phantomsection\label{problem-statement-methodology-overview-slide-3}
\begin{itemize}
\tightlist
\item
  \textbf{Discuss:} The project aims for a \textbf{Comparative
  Study}---comparing models based on two criteria: \textbf{Predictive
  Power} and \textbf{Interpretability}.
\item
  \textbf{Discuss:} The project goal is \textbf{Binary Classification}:
  Building statistical models that are both \textbf{predictive} and
  \textbf{interpretable} for management.
\item
  \textbf{Discuss:} Established an \textbf{80/20 Train/Test Split} and
  used \textbf{5-Fold Cross-Validation (CV)} on the training set for
  tuning and estimating out-of-sample error.
\item
  \textbf{Image/Plot Idea:} An icon representing \textbf{Predictive
  Power} next to an icon representing \textbf{Interpretability} (The two
  core goals).
\end{itemize}
\end{frame}

\begin{frame}{Methodology: Data Preparation (Slide 5)}
\protect\phantomsection\label{methodology-data-preparation-slide-5}
\begin{itemize}
\tightlist
\item
  \textbf{Discuss:} \textbf{Data Wrangling} involved converting
  categorical variables to factors and strategies for handling `unknown'
  levels (treating them as a separate category vs.~missing).
\item
  \textbf{Image/Plot Idea:} A simple flow chart showing: Raw Data
  \(\rightarrow\) Wrangling/Cleaning \(\rightarrow\) Train/Test Split.
\item
  \textbf{Image/Plot Idea:} Summary stats or informational plots
\end{itemize}

{[}Image of a data science workflow diagram{]}
\end{frame}

\begin{frame}[fragile]{LogReg: Feature Selection \& Linearity \#MUSSA}
\protect\phantomsection\label{logreg-feature-selection-linearity-mussa}
\begin{itemize}
\tightlist
\item
  \textbf{Discuss:} Initial feature selection was performed using
  \textbf{Backward Stepwise Elimination} (based on \textbf{AIC}) on the
  full model to find the most parsimonious set of main effects.
\item
  \textbf{Discuss:} \textbf{Deviance Residual Plots} were used to assess
  the linearity assumption for numeric predictors (e.g., \texttt{age},
  \texttt{balance}). This led to testing non-linear terms.
\item
  \textbf{Image/Plot Idea:} The \textbf{Residual Plot} for a key numeric
  feature (e.g., \texttt{balance}), showing a curve suggesting
  non-linearity.
\end{itemize}

\pandocbounded{\includegraphics[keepaspectratio]{Slides_files/figure-beamer/residuals_slide-1.pdf}}
\end{frame}

\begin{frame}{LogReg: Models with nonlinear components and interactions}
\protect\phantomsection\label{logreg-models-with-nonlinear-components-and-interactions}
\begin{itemize}
\tightlist
\item
\end{itemize}
\end{frame}

\begin{frame}{LogReg: Regularization \& Tuning \#MUSSA}
\protect\phantomsection\label{logreg-regularization-tuning-mussa}
\begin{itemize}
\tightlist
\item
  \textbf{Discuss:} We explored \textbf{Ridge (\(\alpha=0\))} and
  \textbf{LASSO (\(\alpha=1\)) Regularization} to manage model
  complexity and feature selection, particularly with the large number
  of dummy variables.
\item
  \textbf{Discuss:} \textbf{5-Fold Cross-Validation} was used to select
  the optimal penalty parameter, \(\lambda\), which minimized the mean
  CV error for each regularized model.
\item
  \textbf{Image/Plot Idea:} A plot showing the \textbf{CV Error
  vs.~\(\text{Log}(\lambda)\)} for the LASSO model, highlighting the
  optimal \(\lambda_{min}\).
\end{itemize}
\end{frame}

\begin{frame}[fragile]{LogReg: Evaluation \& Coefficients \#MUSSA}
\protect\phantomsection\label{logreg-evaluation-coefficients-mussa}
\begin{itemize}
\tightlist
\item
  \textbf{Discuss:} The final selected LogReg model (likely the best
  regularized version) was evaluated on the held-out \textbf{Test Set}.
  Metrics focused on \textbf{AUC} and \textbf{Recall}.
\item
  \textbf{Discuss:} Analyze the \textbf{Coefficients} of the best
  interpretable model, highlighting variables with large magnitudes
  (e.g., \texttt{duration}, \texttt{poutcome}) for actionable insight.
\item
  \textbf{Image/Plot Idea:} The \textbf{Confusion Matrix} for the best
  LogReg model, showing the trade-off between True Positives and False
  Negatives.
\end{itemize}
\end{frame}

\begin{frame}{KNN 1/3: Feature Preparation}
\protect\phantomsection\label{knn-13-feature-preparation}
\begin{itemize}
\tightlist
\item
  \textbf{Discuss:} \textbf{KNN} requires scaling: all numeric
  predictors were \textbf{standardized} (mean=0, SD=1) to prevent
  features with larger scales from dominating the distance calculation.
\item
  \textbf{Discuss:} The non-parametric nature means \textbf{no explicit
  feature selection} via coefficients, but the scaling step is vital for
  ensuring equal contribution from all dimensions.
\item
  \textbf{Image/Plot Idea:} A visual comparison: a scatter plot of two
  features \emph{unscaled} vs.~the same features
  \emph{scaled/standardized}.
\end{itemize}
\end{frame}

\begin{frame}{KNN 2/3: Hyperparameter Tuning}
\protect\phantomsection\label{knn-23-hyperparameter-tuning}
\begin{itemize}
\tightlist
\item
  \textbf{Discuss:} The sole hyperparameter, \textbf{\(k\)} (the number
  of neighbors), determines the model's complexity (i.e., the
  bias-variance trade-off).
\item
  \textbf{Discuss:} \textbf{5-Fold Cross-Validation} was performed
  across a range of \(k\) values to find the optimal \(k\) that
  minimized the mean CV error.
\item
  \textbf{Image/Plot Idea:} A plot showing \textbf{CV Test Error
  vs.~\(k\)}, illustrating the U-shape curve where the optimal \(k\) is
  selected.
\end{itemize}
\end{frame}

\begin{frame}{KNN 3/3: Evaluation \& Test Performance}
\protect\phantomsection\label{knn-33-evaluation-test-performance}
\begin{itemize}
\tightlist
\item
  \textbf{Discuss:} The final KNN model (with optimal \(k\)) was applied
  to the \textbf{Test Set}. Evaluate performance using \textbf{AUC} and
  compare it to the LogReg models.
\item
  \textbf{Discuss:} KNN generally provides a good intermediate
  balance---often capturing non-linearity better than LogReg but being
  less computationally expensive than boosting.
\item
  \textbf{Image/Plot Idea:} A brief table comparing the \textbf{AUC} and
  \textbf{Recall} of the best LogReg model and the final KNN model.
\end{itemize}
\end{frame}

\begin{frame}{Boosting 1/3: Model Formulation}
\protect\phantomsection\label{boosting-13-model-formulation}
\begin{itemize}
\tightlist
\item
  \textbf{Discuss:} \textbf{Boosting} (e.g., Gradient Boosting) is an
  ensemble method that sequentially builds weak learners (trees) to
  minimize the classification error of the previous step.
\item
  \textbf{Discuss:} This model is expected to be our
  \textbf{highest-performing benchmark} but sacrifices interpretability
  due to its complex, additive structure.
\item
  \textbf{Image/Plot Idea:} A conceptual diagram illustrating the
  boosting process (weak learners combining sequentially to form a
  strong learner).
\end{itemize}
\end{frame}

\begin{frame}{Boosting 2/3: Hyperparameter Tuning}
\protect\phantomsection\label{boosting-23-hyperparameter-tuning}
\begin{itemize}
\tightlist
\item
  \textbf{Discuss:} Multiple hyperparameters were tuned using
  \textbf{Cross-Validation}, including the \textbf{number of trees} (M),
  \textbf{interaction depth} (complexity of weak learners), and
  \textbf{learning rate} (\(\eta\)).
\item
  \textbf{Discuss:} The tuning process involves finding the right
  balance between these parameters to ensure the model generalizes well
  without overfitting the training data.
\item
  \textbf{Image/Plot Idea:} A plot showing the \textbf{Error Rate
  vs.~Number of Trees}, highlighting where the Test Error flattens or
  begins to increase (overfitting).
\end{itemize}
\end{frame}

\begin{frame}{Boosting 3/3: Evaluation \& Importance}
\protect\phantomsection\label{boosting-33-evaluation-importance}
\begin{itemize}
\tightlist
\item
  \textbf{Discuss:} The final Boosting model was evaluated on the
  \textbf{Test Set}. Confirm that it achieved the \textbf{highest AUC},
  setting the benchmark for predictive accuracy.
\item
  \textbf{Discuss:} While coefficients are not available, discuss the
  model's \textbf{Variable Importance Plot} as the only glimpse into its
  internal workings.
\item
  \textbf{Image/Plot Idea:} The \textbf{Variable Importance Plot} from
  the Boosting model, which should confirm that the most important
  features align reasonably well with LogReg's significant coefficients.
\end{itemize}
\end{frame}

\begin{frame}{Results: Comparative Evaluation (Slide 13). \#MUSSA}
\protect\phantomsection\label{results-comparative-evaluation-slide-13.-mussa}
\begin{itemize}
\tightlist
\item
  \textbf{Discuss:} Present the \textbf{Final Summary Table} comparing
  the Test Set performance (AUC, Recall, Accuracy) of all models
  (\textbf{Best LogReg}, \textbf{KNN}, \textbf{Boosting}).
\item
  \textbf{Discuss:} Visually compare the models using the
  \textbf{Combined ROC Curve Plot} and discuss which model dominates the
  space.
\item
  \textbf{Image/Plot Idea:} The \textbf{Combined ROC Curve Plot}
  comparing all three final models (LogReg, KNN, and Boosting).
\end{itemize}
\end{frame}

\begin{frame}{The Trade-Off / Business Impact \#MUSSA}
\protect\phantomsection\label{the-trade-off-business-impact-mussa}
\begin{itemize}
\tightlist
\item
  \textbf{Discuss:} Reiterate the central trade-off: \textbf{Boosting}
  is the best \emph{predictor}, but the \textbf{Regularized Logistic
  Regression} offers the most \emph{interpretable} and \emph{actionable}
  insights for the bank.
\item
  \textbf{Discuss:} Final actionable advice for the bank: use
  \textbf{LogReg insights} to design campaigns and \textbf{Boosting} for
  a final scoring model if maximum predictive accuracy is needed.
\item
  \textbf{Image/Plot Idea:} A visual metaphor (e.g., a scale or seesaw)
  contrasting ``High Performance'' vs.~``High Interpretability.''
\item
  \textbf{Discuss:} How the \textbf{chosen model} (Balancing performance
  and interpretability) can lead to \textbf{cost savings} and
  \textbf{higher ROI} for future campaigns.
\item
  \textbf{Discuss:} Briefly touch upon \textbf{regulatory/ethical
  implications} in banking and the need to justify predictions (further
  supporting the value of interpretability).
\item
  \textbf{Image/Plot Idea:} A currency symbol or a bar chart showing the
  conceptual increase in ROI due to better targeting.
\end{itemize}
\end{frame}

\begin{frame}{Discussion: Limitations \& Future Work (Slide 14) \#MUSSA}
\protect\phantomsection\label{discussion-limitations-future-work-slide-14-mussa}
\begin{itemize}
\tightlist
\item
  \textbf{Discuss:} Acknowledge limitations, such as not using
  \textbf{time-series analysis} or fully accounting for the dependency
  of the `duration' feature.
\item
  \textbf{Discuss:} Suggest future improvements: Exploring advanced
  techniques like \textbf{resampling methods} (SMOTE) to handle
  imbalance better or trying \textbf{Deep Learning} models.
\end{itemize}
\end{frame}

\begin{frame}{References (Slide 15)}
\protect\phantomsection\label{references-slide-15}
\begin{itemize}
\tightlist
\item
  \textbf{Discuss:} Simply list the required references for the data
  source and any major methodology papers.
\item
  \textbf{Discuss:} Provide contact information for questions.
\item
  Thank you
\item
  \textbf{Image/Plot Idea:} The official UCI ML Repository logo and your
  contact email.
\end{itemize}
\end{frame}

\end{document}
