---
title: "DSCI 445 Final Report"
subtitle: "Machine Learning Models on A Bank Marketing Dataset"
author: "Sandra Afrifa, Tigist Kefelew , Mussa Hassen, Waddah Alqahtani"
output: pdf_document
header-includes:
  - \usepackage{float}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(readr)
library(tidyverse)
```


```{r, include=FALSE}
library(readr)
library(tidyverse)
library(boot)
library(glmnet)
library(caret)
library(pROC)
library(MASS)
library(ggplot2)
library(dplyr)

set.seed(445) 

bank_full <- read_delim("data/bank-full_1.csv", 
    delim = ";", escape_double = FALSE, trim_ws = TRUE)

bank_full$y <- factor(bank_full$y, levels = c("no", "yes"))


bank_full <- bank_full %>%
  mutate(
    job = as.factor(job),
    marital = as.factor(marital),
    education = as.factor(education),
    default = as.factor(default),
    housing = as.factor(housing),
    loan = as.factor(loan),
    contact = as.factor(contact),
    month = as.factor(month),
    poutcome = as.factor(poutcome)) %>% 
  mutate(previously_contacted = as.factor(ifelse(pdays == -1, "no", "yes")))

## Train/Test split

train_index <- createDataPartition(bank_full$y, p = 0.8, list = FALSE, times = 1)
train_data <- bank_full[train_index, ]
test_data  <- bank_full[-train_index, ]
```


# Motivation

Bank telemarketing campaigns are a major channel through which financial institutions promote term deposit products. These marketing campaigns come with a cost and are often inefficient due to low client response rates. 
Prior work by Moro et al. (2014) discover that client characteristics and campaign related factors strongly influence subscription outcomes. Motivating a data-driven approach to improve targeting and decision-making in bank marketing. 
In their approach a Portuguese retail bank was studied using campaign data collected between 2008 and 2013. 
    
Our dataset was a publicly available dataset, thus had less variables for privacy concerns of the bank customers.
The Bank Marketing dataset was collected by a Portuguese banking institution during a series of direct telemarketing campaigns. These campaigns were done to encourage clients to subscribe to a long-term deposit product. Telemarketing is an expensive and time-consuming process; therefore, the bank would want to know in advance which customers are most likely to subscribe before any more investment takes place.
The dataset consists of sixteen variables describing demographic, financial and campaign related variables that enlist individual clients contacted during direct marketing efforts. A list of the 16 variable found below...

* age: Age in years 
* job: Occupation (Categorical)
* marital: Marital status (Categorical)
* education: Highest level of education (Categorical)
* default: has credit in default? (binary)
* balance: average yearly balance
* housing: has housing loan? (binary)
* loan: has personal loan? (binary)
* Contract: contact communication type (Categorical)
* day: last contact day (of the month)
* month: last contact month of year (month; 1=January)
* duration: last contact duration, in seconds
* campaign: number of contacts performed during this campaign and for this client
* pdays: number of days that passed by after the client was last contacted from a previous campaign (-1 means client was not previously contacted)
* previous: number of contacts performed before this campaign and for this client
* poutcome: outcome of the previous marketing campaign (Categorical)
* y: Indicator of whether the client subscribed a term deposit (Binary target) 


The response variable indicates whether a client subscribed to a term deposit which defines a binary classification task. 

Exploratory analysis shows that the numeric predictors (balance, duration, pdays, previously contacted) are heavily skewed.
Also, analysis reveals substantial class imbalance, in the response variable, which motivates the use of appropriate modeling and evaluation for our classification problem.
Visualizations of subscription rates by job education marital status contact type and previous outcome reveal variations in client responsiveness across different factors.

## Exploratory Data Analysis (EDA)

### Data Characteristics and Class Imbalance

Our exploratory analysis examined 45,211 client records from the Portuguese banking institution. Analysis of the target variable revealed severe class imbalance, with only 11.7% of clients subscribing to term deposits compared to 88.3% who declined (Figure 1). This imbalance represented a key challenge for predictive modeling.

```{r fig1, echo=FALSE, fig.cap="Class imbalance in term deposit subscriptions. Only 11.7% of clients subscribed, presenting a significant modeling challenge.", fig.align='center', out.width='70%', fig.pos='H'}

ggplot(bank_full, aes(x = y, fill = y)) +
  geom_bar(color = "white", linewidth = 0.4) +
  scale_fill_manual(
    values = c("no" = "grey75", "yes" = "seagreen3")
  ) +
  labs(
    x = "Subscribed?",
    y = "Count"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    legend.position = "none",
    panel.grid.major.x = element_blank()
  )

```


### Demographic Patterns

Demographic patterns emerged from the data: retired clients and students were most likely to subscribe, while blue-collar workers were least likely. Higher education levels correlated with increased subscription rates.

```{r fig2, echo=FALSE, fig.cap="Distribution of job categories in the dataset. Administrative, blue-collar, and technician occupations are most common.", fig.align='center', out.width='70%', fig.pos='H'}
      
ggplot(bank_full, aes(x = job)) +
  geom_bar(fill = "steelblue3") +
  coord_flip() +
  labs(
    x = "Job",
    y = "Count"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    panel.grid.major.y = element_blank()
  )

```


### Historical Patterns

Analysis of previous campaign outcomes revealed powerful patterns: clients with previous successful subscriptions demonstrated approximately 65% likelihood of subscribing again, compared to only 15% among those with previous unsuccessful outcomes.

```{r fig3, echo=FALSE, fig.cap="Previous campaign outcome influence. Clients with previous success show 65% subscription rates compared to 15% for previous failures.", fig.align='center', out.width='70%', fig.pos='H'}

ggplot(bank_full, aes(x = poutcome, fill = y)) +
  geom_bar(position = "fill", color = "white", linewidth = 0.3) +
  scale_fill_manual(
    values = c("no" = "grey75", "yes" = "seagreen3")
  ) +
  scale_y_continuous(labels = scales::percent) +
  labs(
    x = "Previous Outcome",
    y = "Proportion Subscribed"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    legend.position = "none"
  )

```

### Campaign Dynamics and Diminishing Returns

Campaign persistence analysis showed a clear downward trend in success rates as contact frequency increased, with substantial decline after five attempts and minimal success beyond thirty contacts.

```{r fig4, echo=FALSE, fig.cap="Diminishing returns from repeated contacts. Success rates drop significantly after 5 contacts.", fig.align='center', out.width='70%', fig.pos='H'}

bank_full %>%
  group_by(campaign) %>%
  summarise(success_rate = mean(y == "yes")) %>%
  ggplot(aes(x = campaign, y = success_rate)) +
  geom_line(color = "steelblue3", linewidth = 1.2) +
  geom_point(color = "darkorange2", size = 2.8) +
  scale_y_continuous(labels = scales::percent) +
  labs(
    x = "Number of Contacts",
    y = "Success Rate"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    panel.grid.minor = element_blank()
  )

```


Data preprocessing involved converting categorical variables to factors and adding derived variables such as pdays to account for clients not previously contacted. Converting the categorical variables (initilay character type) to factors creates an indicator for each class in the variable (i.e. if job has 20 factors, 20 indicator variables will be considered in the model fit). As for the previously contaced variable (days since previously contacted), customers who haven't been prreviously contacted will recieve a value of -1. This can cause improper modeling of customers, since the variable now takes on the value of negative numbers. Instead we set -1 to zeros and created a new variable to indicate customers that have not been previously coontacted before. These steps ensure that the dataset is suitable for statistical learning models and enable accurate assessment of predictive performance. \

## Problem Statement     
This study is motivated by the availability of bank marketing datasets that enable comparison of machine learning methods for predicting term deposit subscription behavior.

The specification of the problem begins with the recognition that we are addressing a binary classification task where the goal is to predict whether a client will subscribe to a term deposit based on demographic financial and campaign features. According to the framework presented in Introduction to Statistical Learning (ISLR), the fundamental goal is to estimate the unknown function, f(x) that relates the predictors, X to the response, Y. Some models are highly flexible and can capture complex non-linear relationships between predictors and the response. In most Cases these models achieve higher predictive accuracy. However, these flexible models often suffer from lower interpretability, making it difficult to understand the influence of individual features. Simpler models such as logistic regression are less flexible but provide coefficients that can be directly interpreted in terms of odds ratios, offering insight into feature importance. 

Chapter 5 of Introduction to Statistical Learning emphasizes the importance of obtaining an unbiased estimate of a model’s test error to assess its predictive performance on unseen data. Test error quantifies how well a model generalizes beyond the data used for training, which is critical for selecting among competing models. The chapter presents several methods for estimating test error, including the simple training/test split, K-fold cross validation, and the leave-one-out cross validation (LOOCV) approach. Each method balances bias and variance differently, with cross validation generally providing a lower-variance estimate compared to a single split. These techniques are essential for model selection, tuning hyperparameters, and comparing the expected performance of alternative models. For the Bank Marketing dataset, estimating test error accurately is particularly important due to the class imbalance and the presence of both categorical and numeric predictors, which can influence model generalization.

In the following chapter of Introduction to Statistical Learning focuses on model selection and regularization techniques to improve predictive performance and reduce overfitting. It introduces methods such as subset selection, shrinkage (Ridge and LASSO regression), and dimension reduction to assess and enhance model accuracy while controlling variance. These approaches allow practitioners to identify parsimonious models that generalize well to new data, providing complementary strategies to traditional test error estimation. We apply these concepts to the models we fit on the bank data, to reduce overfitting and ultimately improve the generalization of our models to new clients. \

We start with a logistic regression (described in methodology section), then build up adjustments and regularization to the logestic regression to make our baseline. We then move to another interpretable model: KNN. This model is considered transparent in a unique, due to it's neighboring factor.
Finally we fit a Boosting model. This state-of-the-art model is not considered interpretable. Although, there are methods used to extract the contributing variables in such models. We included this model is used to gage how much accuracy we are sacrificing for interpretability. 

The authors in (Moro et al.) employed time ordered data splitting rolling window evaluation. For feature enrichment, they implemented a semi-automatic approach including external intuitive knowledge from domain experts (bank manager) to optimize features. They compared Logistic regression with Decision trees, SVMs, and Neural networks; all complex 'black-box' models. While (Moro et al.) prioritized maximizing classification performance, our approach also examines model simplicity, automatic feature selection, and diagnostic checks for appropriate classifications (from confusion matrices).


# Methodology
For each of our models we implement a k-fold CV approach, with k=5 training/test split to maintain the distribution of the response variable across the sets. Our models do not apply time-series modeling, since our data does not include sequential variables. Although variables such as campaign month or day are available within the dataset, which can be considered to hold temporal patterns, we do not model this data as a time series. This keeps the focus on cross-sectional relationships between client characteristics and subscription behavior. An 80/20 random train–test split was used, and all modeling decisions, including feature selection and hyper-parameter tuning, were made using only the training data. This prevented information leakage and ensured that the test set provided an honest evaluation of generalization performance.

1. **Logistic regression**

```{r, include=FALSE}
full_formula <- y ~ age + job + marital + education + default + balance + 
  housing + loan + contact + day + month + duration + campaign + 
  pdays + previous + poutcome + previously_contacted

glm.full <- glm(full_formula, data = train_data, family = "binomial")

# Perform backward selection
glm.step <- step(glm.full, direction = "backward", trace = 0)


selected_formula <- formula(glm.step)


## 
glm.baseline <- glm(selected_formula, data = train_data, family = "binomial")
summary(glm.baseline)

ctrl <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE, 
  summaryFunction = twoClassSummary 
)

set.seed(445) 
cv_fit <- train(
  selected_formula, 
  data = train_data, 
  method = "glm", 
  family = "binomial",
  trControl = ctrl, 
  metric = "ROC" 
)

test_probabilities <- predict(glm.baseline, newdata = test_data, type = "response")
test_predictions <- factor(ifelse(test_probabilities > 0.5, "yes", "no"), levels = levels(test_data$y))

conf_matrix <- confusionMatrix(data = test_predictions, reference = test_data$y, positive = "yes" )



# Extract deviance residuals
residuals <- residuals(glm.baseline, type = "deviance")
train_data$residuals <- residuals

p1 <- ggplot(train_data, aes(x = balance, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "blue") +
  labs(title = "Deviance Residuals vs. Balance",
       y = "Deviance Residuals") +
  theme_minimal()

glm.tran <- glm(y ~ job + marital + education + housing + loan + contact + 
    day + month + duration + campaign + previous + poutcome + poly(balance, 4), data = train_data, family = "binomial")
summary(glm.tran)
tran.coef <- summary(glm.tran)$coef

residuals <- residuals(glm.tran, type = "deviance")
train_data$residuals <- residuals

p2 <- ggplot(train_data, aes(x = balance, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "blue") +
  labs(title = "Deviance Residuals vs. Balance",
       y = "Deviance Residuals") +
  theme_minimal()


## Regularization
X_full <- model.matrix(y ~ . - 1, data = bank_full)
Y_full <- as.numeric(bank_full$y) - 1 

X_train <- X_full[train_index, ]
Y_train <- Y_full[train_index] 

X_test <- X_full[-train_index, ]
Y_test <- Y_full[-train_index] 


cv.ridge <- cv.glmnet(X_train, Y_train, 
                      alpha = 0, 
                      family = "binomial", 
                      type.measure = "class")
lambda.min.ridge <- cv.ridge$lambda.min

cv.lasso <- cv.glmnet(X_train, Y_train, 
                      alpha = 1, 
                      family = "binomial", 
                      type.measure = "class")
lambda.min.lasso <- cv.lasso$lambda.min

baseline_prob <- predict(glm.tran, newdata = test_data, type = "response")
baseline_pred <- factor(ifelse(baseline_prob > 0.5, "yes", "no"), levels = c("no", "yes"))

ridge_prob <- predict(cv.ridge, s = lambda.min.ridge, newx = X_test, type = "response")
ridge_pred <- factor(ifelse(ridge_prob > 0.5, "yes", "no"), levels = c("no", "yes"))

lasso_prob <- predict(cv.lasso, s = lambda.min.lasso, newx = X_test, type = "response")
lasso_pred <- factor(ifelse(lasso_prob > 0.5, "yes", "no"), levels = c("no", "yes"))


cm_baseline <- confusionMatrix(baseline_pred, test_data$y, positive = "yes")
cm_ridge <- confusionMatrix(ridge_pred, test_data$y, positive = "yes")
cm_lasso <- confusionMatrix(lasso_pred, test_data$y, positive = "yes")

metrics <- data.frame(
  Model = c("Baseline Logit", "Ridge Logit", "LASSO Logit"),
  Accuracy = c(cm_baseline$overall['Accuracy'], cm_ridge$overall['Accuracy'], cm_lasso$overall['Accuracy']),
  Sensitivity = c(cm_baseline$byClass['Sensitivity'], cm_ridge$byClass['Sensitivity'], cm_lasso$byClass['Sensitivity']),
  Specificity = c(cm_baseline$byClass['Specificity'], cm_ridge$byClass['Specificity'], cm_lasso$byClass['Specificity'])
)


roc_baseline <- roc(test_data$y, baseline_prob)
roc_ridge <- roc(test_data$y, as.numeric(ridge_prob))
roc_lasso <- roc(test_data$y, as.numeric(lasso_prob))

roc_data <- data.frame(
  fpr = c(1 - roc_baseline$specificities, 1 - roc_ridge$specificities, 1 - roc_lasso$specificities),
  tpr = c(roc_baseline$sensitivities, roc_ridge$sensitivities, roc_lasso$sensitivities),
  Model = factor(c(rep("Baseline Logit", length(roc_baseline$specificities)),
                   rep("Ridge Logit", length(roc_ridge$specificities)),
                   rep("LASSO Logit", length(roc_lasso$specificities))),
                 levels = c("Baseline Logit", "Ridge Logit", "LASSO Logit"))
)

```

Logistic regression is used to model the probability of a binary outcome as a function of predictor variables. The model estimates the probability that a client subscribes to a term deposit, $P(Y=1 \mid X)$. This model fits with assumptions of independent observations and normality.

Formula: $$f(x) = P(Y=1 \mid \mathbf{x}) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \dots + \beta_k x_k)}}$$  
Coefficients are estimated using the maximum likelihood method, which finds the parameter values that maximize the likelihood of observing the training data (ISLR). 

### Interpretability
To understand the meaning of the coefficients, we can transform the formula into the log-odds (logit) form:
$$ \log\left(\frac{P(Y=1 \mid \mathbf{x})}{1 - P(Y=1 \mid \mathbf{x})}\right) = \beta_0 + \beta_1 x_1 + \dots + \beta_k x_k$$
Consider a single coefficient, say $\beta_{balance}$, which relates to a client's average bank account balance. If the estimated coefficient for balance is $\beta_{balance} = 0.001$, this means that a one-unit increase in the client's balance is associated with a change of 0.001 in the log-odds of subscribing to a term deposit, assuming all other variables remain constant. This change in log-odds translates to a multiplicative change of $e^{0.001} \approx 1.001$ in the odds. Therefore, for every unit increase in balance, the odds of subscription increase by a factor of 1.001. This shows a clear relationship between the coefficient and the target variable.

Without loss of generality, we can generalize this interpretation and estimation process to logistic regression models with multiple variables and their variations, such as incorporating Lasso regularization (for variable selection and complexity control) or applying polynomial transformations to the predictor variables (to capture non-linear relationships).

### Model fit

Feature selection is performed using backward stepwise selection to identify a parsimonious subset of predictors that minimizes estimated test error as measured by AIC. Variables such as duration and previous marketing outcomes remain in the model due to their strong predictive power, whereas less informative variables are excluded to reduce variance. Categorical predictors are converted to factors, and a new binary variable previously_contacted is created to handle clients with no prior campaign contact. This preprocessing ensures that the data is suitable for statistical modeling and facilitates accurate interpretation of model coefficients. We resulted 12 features:

```{r}
selected_formula
```

Cross validation is used to select optimal tuning parameters for these regularized models and to estimate the expected test error. Confusion matrices, sensitivity, specificity, and ROC/AUC metrics are computed to assess predictive performance, especially given the class imbalance where non-subscribers are the majority. This model performed will with mean 5-fold CV test error metrics:

```{r, echo=FALSE}
print(conf_matrix)
```

Next, we check for potential non-linearity in continuous predictors from our predictors remaining after back-ward stepwise selection. We plotted deviance residuals against numeric features.
Based on residual diagnostics, we incorporated a polynomial term (using the poly function: *poly(balance, 4)*) for balance to account for non-linear effects. The Balance squared and power 4 term produced significant coefficients (Last 4 on the list of coefficients).

```{r, out.width="45%", echo=FALSE}
p1
p2
```

```{r, echo=FALSE}
full_coefs <- summary(glm.tran)$coefficients
ballance_coefs <- c("poly(balance, 4)1", "poly(balance, 4)2", "poly(balance, 4)3", "poly(balance, 4)4")
subset_coefs <- full_coefs[ballance_coefs, c(1, 4)]

print(subset_coefs)
```

Although this polynomial transformation didn't provide desired metrics in expense of the nonlinear effect. From the deviance residual plots we can observe there weren't significant deviance from a normal residual line. We will not include this model in our comparison.


Regularization techniques such as Ridge ($\ell_2$ penalty) and LASSO ($\ell_1$ penalty) are employed to reduce variance and perform feature selection, particularly in the presence of correlated predictors. LASSO (Least Absolute Shrinkage and Selection Operator) regression uses an $\ell_1$ penalty ($\alpha=1$). A key property of the LASSO is that it forces the coefficients of some variables to be exactly zero, thereby performing automatic feature selection (ISLR Chapter 6). The LASSO plot similarly uses cross-validation to select the optimal $\lambda_{\min}$, `r round(lambda.min.lasso, 5)`. Unlike Ridge, the LASSO is expected to yield a more parsimonious model by excluding irrelevant predictors entirely. 

```{r, out.width="45%", echo=FALSE}
plot(cv.ridge)
plot(cv.lasso)
```

Here is a comparison of the models fitted thus far:

```{r}
metrics
```

Ridge and LASSO models yield similar results, indicating that additional shrinkage does not meaningfully improve classification of subscribers. We assume this is due to our modeling being majority indicator variables. This binary aspect of the indicator variable causes it's coefficient to be less affected by regularization techniques.

```{r, out.width="45%", echo=FALSE}
ggplot(roc_data, aes(x = fpr, y = tpr, color = Model)) +
  geom_line(linewidth = 1) +
  geom_abline(linetype = "dashed", color = "grey") +
  labs(
    title = "ROC Curve Comparison for Logistic Models",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  theme_minimal()
```

All three models exhibit a high overall Accuracy (around 0.89), which is expected given the significant class imbalance (Prevalence $\approx 0.11$). However, the critical metric, Sensitivity (the True Positive Rate for the minority class, 'yes'), is very low (around 0.17). This result, consistent across all three models, indicates that they struggle to correctly identify subscribers, leading to many False Negatives (FN). Conversely, the high Specificity (True Negative Rate $\approx 0.98$) means the models are excellent at identifying non-subscribers. The regularization methods (Ridge and LASSO) failed to provide any significant improvement in these metrics, suggesting that the primary limitation is one of high bias (due to insufficient flexibility) rather than high variance.
\

### Models Considered:

We evaluated five main models from the course: 


2. **K-Nearest Neighbors (KNN)**

- K-Nearest Neighbors (KNN) — A nonparametric method that makes predictions by finding the k most similar clients. It captures nonlinear relationships but can be sensitive to scaling and high-dimensional data.

K-Nearest Neighbors is a nonparametric classification method that predicts an outcome for a new observation by looking at the k most similar clients in the training dataset. Rather than estimating a set of coefficients or assuming any functional form between predictors and the response, KNN makes predictions based purely on proximity in predictor space. This makes the method particularly well-suited when the underlying relationship between predictors and subscription behavior is nonlinear or too complicated for a parametric model to capture.

Because KNN relies on distance calculations, feature scaling is an important preprocessing step. Otherwise, predictors that are measured on a larger scale-e.g., account balance or call duration-would dominate the distance metric and distort which observations are considered "nearest." In our implementation, we standardized all numeric variables so that each feature was given equal weight in determining the similarity between any two customers. This preprocessing step is especially important for marketing datasets which often involve heterogeneous variables.
A major modeling decision in KNN, which involves a choice, is the value of k. For small values of k, the classifier is highly flexible and closely follows the training data with a potential risk of overfitting noise. Larger values produce smoother, more stable decision boundaries; however, this has a potential risk of missing the important patterns. We used 5-fold cross-validation to identify the value of k that minimized the estimated test error, balancing the bias-variance trade-off.

While KNN presents an intuitive method of finding clients with similar profiles, it has several limitations in this context. The performance may suffer for high-dimensional data, with points becoming further apart and distances becoming less informative-the "curse of dimensionality". Second, KNN is less interpretable than models like logistic regression, where the effect of every predictor can be expressed quantitatively. It does, however, provide a useful baseline when assessing the performance of alternative nonlinear classification methods for predicting term deposit subscriptions.

```{r}
# KNN Model
library(class)

# Remove any extra columns that might have been added
train_data_clean <- train_data %>% 
  dplyr::select(-any_of(c("residuals", "y_num")))
test_data_clean <- test_data %>% 
  dplyr::select(-any_of(c("residuals", "y_num")))

# Prepare data for KNN (standardize numeric variables)
numeric_vars <- c("age", "balance", "day", "duration", "campaign", "pdays", "previous")

# Create model matrices from combined data to ensure same dimensions
full_data_temp <- rbind(
  train_data_clean %>% mutate(set = "train"),
  test_data_clean %>% mutate(set = "test")
)

# Standardize numeric variables
full_data_temp[numeric_vars] <- scale(full_data_temp[numeric_vars])

# Create model matrix for all data at once
X_full_knn <- model.matrix(y ~ . - set - 1, data = full_data_temp)

# Split back into train and test
X_train_knn <- X_full_knn[full_data_temp$set == "train", ]
X_test_knn <- X_full_knn[full_data_temp$set == "test", ]

# Find optimal k using cross-validation
set.seed(445)
k_values <- c(3, 5, 7, 9, 11, 15, 20, 25, 30)

# Use caret for easier CV with KNN
ctrl_knn <- trainControl(method = "cv", number = 5)

set.seed(445)
knn_cv <- train(
  y ~ .,
  data = train_data_clean,
  method = "knn",
  trControl = ctrl_knn,
  tuneGrid = data.frame(k = k_values)
)

best_k <- knn_cv$bestTune$k

# Fit final KNN model
knn_pred <- knn(
  train = X_train_knn,
  test = X_test_knn,
  cl = train_data_clean$y,
  k = best_k,
  prob = TRUE
)

# Get probabilities
knn_prob <- attr(knn_pred, "prob")
knn_prob <- ifelse(knn_pred == "yes", knn_prob, 1 - knn_prob)

# Confusion matrix
cm_knn <- confusionMatrix(knn_pred, test_data_clean$y, positive = "yes")

# ROC curve
roc_knn <- roc(test_data_clean$y, knn_prob)
```

```{r, out.width="45%",}
# Confusion Matrix
cm_knn

# Best k value
cat("\nOptimal number of neighbors (k):", best_k, "\n")
cat("Cross-validation accuracy:", round(max(knn_cv$results$Accuracy), 4), "\n")
```

```{r, out.width="45%",}
# CV results plot
plot(knn_cv, main = "KNN Cross-Validation Performance")
```

```{r, out.width="45%",}
# ROC curve
plot(roc_knn, col = "steelblue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "gray")
text(0.6, 0.3, paste("AUC =", round(auc(roc_knn), 4)), cex = 1.2)
```

3. **Boosting**

Boosting was used as the most flexible modeling approach in this study to address the strong class imbalance and complex predictor relationships present in the Bank Marketing dataset. Unlike logistic regression and KNN, boosting does not rely on a fixed functional form and can naturally capture nonlinear effects and interactions among demographic, financial, and campaign-related variables.

We implemented gradient boosting using a Bernoulli loss function for binary classification. The model was trained on the same 80% training set as the other methods. To encourage gradual learning and prevent overfitting, we used a small learning rate (shrinkage = 0.01) and shallow trees (interaction depth = 3). The model was initially trained with 1,500 trees to allow sufficient flexibility.

The optimal number of trees was selected using out-of-bag (OOB) error estimation, which serves a similar role to cross-validation in ensemble methods. The OOB procedure identified the point at which additional trees no longer meaningfully improved predictive performance. Final predictions were computed using the optimal number of trees selected by this procedure.

```{r, out.width="45%", boosting-fit, echo=FALSE, fig.cap="Out-of-bag error used to select the optimal number of boosting trees.", fig.pos='H'}
library(gbm)

train_data <- train_data %>% dplyr::select(-any_of("residuals"))
test_data  <- test_data  %>% dplyr::select(-any_of("residuals"))

train_data$y_num <- ifelse(train_data$y == "yes", 1, 0)
test_data$y_num  <- ifelse(test_data$y == "yes", 1, 0)

set.seed(445)
boost_model <- gbm(
  y_num ~ . - y,
  data = train_data,
  distribution = "bernoulli",
  n.trees = 1500,
  shrinkage = 0.01,
  interaction.depth = 3,
  bag.fraction = 0.5,
  verbose = FALSE
)

best_trees <- gbm.perf(boost_model, method = "OOB", plot.it = TRUE)

```


```{r, out.width="45%",}
boost_prob <- predict(boost_model, test_data, n.trees = best_trees, type = "response")
boost_pred <- factor(ifelse(boost_prob > 0.5, "yes", "no"), levels = c("no","yes"))

cm_boost <- confusionMatrix(boost_pred, test_data$y, positive = "yes")
cm_boost

```


```{r, out.width="45%",}
roc_boost <- roc(test_data$y, boost_prob)
plot(roc_boost, col = "darkgreen", lwd = 2)
abline(a = 0, b = 1, lty = 2)

```

When evaluated on the test set, boosting achieved the strongest overall performance among all models considered. It produced the highest accuracy (0.905), sensitivity (0.412), and AUC (0.927). In particular, boosting correctly identified over 40% of subscribing clients while maintaining high specificity. This represents a substantial improvement over logistic regression, KNN, and GAM, all of which struggled to identify the minority “yes” class. These results highlight the advantage of flexible ensemble methods in reducing bias and capturing complex nonlinear patterns in imbalanced marketing data.

### Model Training, Formulation, and Validation

K-Nearest Neighbors required additional preprocessing due to its reliance on distance-based calculations. All numeric predictors were standardized to ensure equal contribution to the distance metric. The number of neighbors, 
k, was selected using 5-fold cross-validation by identifying the value that minimized classification error on the training folds.

Boosting was trained using a large number of trees to allow sufficient model flexibility, with the optimal number of trees selected using out-of-bag error estimation. This tuning procedure served a similar role to cross-validation by identifying the point at which additional complexity no longer improved performance.
Across all models, final performance was assessed on the held-out test set using accuracy, sensitivity, and AUC. Because the dataset is highly imbalanced, particular emphasis was placed on sensitivity and AUC as measures of a model’s ability to identify the minority “yes” class.

# Results

Model performance was evaluated on a held-out test set using accuracy, sensitivity, and AUC. Because the dataset is highly imbalanced, particular emphasis was placed on sensitivity and AUC as measures of a model’s ability to identify the minority “yes” class.

```{r}
# Create final results comparison table
final_results <- data.frame(
  Model = c("Baseline Logit", "Ridge Logit", "LASSO Logit", "KNN", "Boosting"),
  Accuracy = c(
    cm_baseline$overall['Accuracy'],
    cm_ridge$overall['Accuracy'],
    cm_lasso$overall['Accuracy'],
    cm_knn$overall['Accuracy'],  # You'll need to create this
    cm_boost$overall['Accuracy']
  ),
  Sensitivity = c(
    cm_baseline$byClass['Sensitivity'],
    cm_ridge$byClass['Sensitivity'],
    cm_lasso$byClass['Sensitivity'],
    cm_knn$byClass['Sensitivity'],  # You'll need to create this
    cm_boost$byClass['Sensitivity']
  ),
  AUC = c(
    auc(roc_baseline),
    auc(roc_ridge),
    auc(roc_lasso),
    auc(roc_knn),  # You'll need to create this
    auc(roc_boost)
  )
)

# Round for better display
final_results[, 2:4] <- round(final_results[, 2:4], 4)
```


```{r}
final_results
```


Boosting outperformed all other models across all key performance metrics. It achieved the highest accuracy (0.9047), sensitivity (0.4115), and AUC (0.9266), indicating superior ability to identify clients likely to subscribe to a term deposit. This improvement in sensitivity is particularly important in the bank marketing context, where failing to identify potential subscribers represents a missed business opportunity.

The logistic regression models (baseline, Ridge, and LASSO) achieved similar accuracy levels (approximately 0.898–0.900) and strong AUC values (around 0.907), indicating good overall discrimination. However, their sensitivity remained relatively low, suggesting that these models frequently misclassified subscribing clients as non-subscribers. Regularization did not substantially improve performance, indicating that model bias rather than variance was the primary limitation.

KNN performed the weakest among all models, with the lowest AUC (0.617), reflecting poor discriminatory power in this high-dimensional setting. GAM demonstrated improved AUC (0.854) but still failed to substantially improve sensitivity, indicating limited gains from smooth nonlinear terms alone.

```{r, out.width="45%",}
# Combine all ROC data for plotting
roc_all <- data.frame(
  fpr = c(
    1 - roc_baseline$specificities,
    1 - roc_ridge$specificities,
    1 - roc_lasso$specificities,
    1 - roc_knn$specificities,
    1 - roc_boost$specificities
  ),
  tpr = c(
    roc_baseline$sensitivities,
    roc_ridge$sensitivities,
    roc_lasso$sensitivities,
    roc_knn$sensitivities,
    roc_boost$sensitivities
  ),
  Model = factor(
    c(
      rep("Baseline Logit", length(roc_baseline$specificities)),
      rep("Ridge Logit", length(roc_ridge$specificities)),
      rep("LASSO Logit", length(roc_lasso$specificities)),
      rep("KNN", length(roc_knn$specificities)),
      rep("Boosting", length(roc_boost$specificities))
    ),
    levels = c("Baseline Logit", "Ridge Logit", "LASSO Logit", "KNN", "Boosting")
  )
)
```

```{r, out.width="45%",}
ggplot(roc_all, aes(x = fpr, y = tpr, color = Model)) +
geom_line(linewidth = 1) +
geom_abline(linetype = "dashed") +
labs(
x = "False Positive Rate",
y = "True Positive Rate"
) +
theme_minimal()

```

The ROC curves reinforce the numerical results. Boosting dominates across most thresholds, confirming its superior ranking ability. Logistic regression models cluster closely together, while KNN performs only slightly better than random classification.


# Discussion

The results of this study highlight an important trade-off between predictive performance and interpretability when modeling imbalanced bank marketing data. Among all models evaluated, boosting achieved the strongest overall predictive performance, particularly in terms of sensitivity and AUC. This indicates that boosting is the most effective method for identifying clients who are likely to subscribe to a term deposit, which is the primary objective of the bank. By capturing complex nonlinear relationships and interactions among predictors, boosting substantially reduced the number of missed potential subscribers compared to simpler models.

However, while boosting offers superior predictive accuracy, it lacks transparency. In contrast, regularized logistic regression provides more interpretable results that can be directly translated into business insights. Coefficient estimates from the logistic models reveal how specific client characteristics influence subscription probability. For example, higher account balances were associated with lower marginal increases in subscription probability at extreme values, housing loan status was negatively associated with subscription, and student job status showed a strong positive association. These interpretable effects allow marketing teams to better understand customer behavior and design targeted strategies, even if the model itself is not the most accurate.

Interpretability is particularly important in the banking sector, where predictive decisions may be subject to regulatory oversight and ethical scrutiny. Models that can clearly justify why a client was targeted or excluded are easier to audit and defend. From this perspective, logistic regression remains a valuable tool despite its lower sensitivity. A practical strategy for the bank may involve using boosting as a primary screening model to identify high-potential clients, followed by logistic regression to provide explanations and support decision-making.
Several limitations should be noted. The analysis did not incorporate time-aware modeling, even though campaign timing may influence client responses. Additionally, no resampling techniques were used to directly address class imbalance. Future work could explore methods such as SMOTE or cost-sensitive learning to further improve minority-class detection. Other extensions may include experimenting with alternative ensemble methods or adjusting decision thresholds to better align predictions with business costs.

Overall, this study demonstrates that flexible ensemble models offer substantial gains in predictive performance for imbalanced marketing problems, while interpretable models remain essential for transparency and actionable insight. Balancing these two objectives is critical for effective and responsible deployment of predictive models in real-world banking applications.


## References

Dua, D., & Graff, C. (2019). UCI Machine Learning Repository: Bank Marketing Dataset. 
https://archive.ics.uci.edu/dataset/222/bank+marketing

James, Gareth, et al. An Introduction to Statistical Learning : With Applications in R. Springer, 2013.

Moro, Sérgio, et al. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, vol. 62, June 2014, pp. 22–31.
