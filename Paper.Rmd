---
title: "DSCI 445 Final Report"
subtitle: "Machine Learning Models on A Bank Marketing Dataset"
author: "Sandra Afrifa, Tigist Kefelew , Mussa Hassen, Waddah Alaahtani"
output: pdf_document
---

```{r, include=FALSE}
library(readr)
library(tidyverse)
library(boot)
library(glmnet)
library(caret)
library(pROC)
library(MASS)
library(ggplot2)
library(dplyr)

set.seed(445) 

bank_full <- read_delim("data/bank-full_1.csv", 
    delim = ";", escape_double = FALSE, trim_ws = TRUE)

bank_full$y <- factor(bank_full$y, levels = c("no", "yes"))


bank_full <- bank_full %>%
  mutate(
    job = as.factor(job),
    marital = as.factor(marital),
    education = as.factor(education),
    default = as.factor(default),
    housing = as.factor(housing),
    loan = as.factor(loan),
    contact = as.factor(contact),
    month = as.factor(month),
    poutcome = as.factor(poutcome)) %>% 
  mutate(previously_contacted = as.factor(ifelse(pdays == -1, "no", "yes")))

## Train/Test split

train_index <- createDataPartition(bank_full$y, p = 0.8, list = FALSE, times = 1)
train_data <- bank_full[train_index, ]
test_data  <- bank_full[-train_index, ]
```


## Motivation

Bank telemarketing campaigns are a major channel through which financial institutions promote term deposit products. Thsee marketing campaigns come with a cost and are often inefficient due to low client response rates. 
Prior work by Moro et al. (2014) discover that client characteristics and campaign related factors strongly influence subscription outcomes. Motivating a data-driven approach to improve targeting and decision-making in bank marketing. 
In their approach a Portuguese retail bank was studied using campaign data collected between 2008 and 2013. 
    
Our dataset was a publicily available dataset, thus had less variables for privacy concerns of the bank customers.
The Bank Marketing dataset was collected by a Portuguese banking institution during a series of direct telemarketing campaigns. These campaigns were done to encourage clients to subscribe to a long-term deposit product. Telemarketing is an expensive and time-consuming process; therefore, the bank would want to know in advance which customers are most likely to subscribe before any more investment takes place.
The dataset consists of sixteen variables describing demographic, financial and campaign related variables that enlist individual clients contacted during direct marketing efforts. A list of the 16 variable found below...

* age: Age in years 
* job: Occupation (Categorical)
* marital: Marital status (Categorical)
* education: Highest level of education (Categorical)
* default: has credit in default? (binary)
* balance: average yearly balance
* housing: has housing loan? (binary)
* loan: has personal loan? (binary)
* Contract: contact communication type (Categorical)
* day: last contact day (of the month)
* month: last contact month of year (month; 1=January)
* duration: last contact duration, in seconds
* campaign: number of contacts performed during this campaign and for this client
* pdays: number of days that passed by after the client was last contacted from a previous campaign (-1 means client was not previously contacted)
* previous: number of contacts performed before this campaign and for this client
* poutcome: outcome of the previous marketing campaign (Categorical)
* y: Indicator of whether the client subscribed a term deposit (Binary target) 


The response variable indicates whether a client subscribed to a term deposit which defines a binary classification task. 

Exploratory analysis shows that the numeric predictors (balance, duration, pdays, previously contacted) are heavily skewed.
Also, analysis reveals substantial class imbalance, in the response variable, which motivates the use of appropriate modeling and evaluation for our classification problem.
Visualizations of subscription rates by job education marital status contact type and previous outcome reveal variations in client responsiveness across different factors.

*[STREAMLINE Plots and EDA summary HERE]*

Data preprocessing involved converting categorical variables to factors and adding derived variables such as pdays to account for clients not previously contacted. Converting the categorical variables (initilay character type) to factors creates an indicator for each class in the variable (i.e. if job has 20 factors, 20 indicator variables will be considered in the model fit). As for the previously contaced variable (days since previously contacted), customers who haven't been prreviously contacted will recieve a value of -1. This can cause improper modeling of customers, since the variable now takes on the value of negative numbers. Instead we set -1 to zeros and created a new variable to indicate customers that have not been previously coontacted before. These steps ensure that the dataset is suitable for statistical learning models and enable accurate assessment of predictive performance. \

### Problem Statement     
This study is motivated by the availability of bank marketing datasets that enable comparison of machine learning methods for predicting term deposit subscription behavior.

The specification of the problem begins with the recognition that we are addressing a binary classification task where the goal is to predict whether a client will subscribe to a term deposit based on demographic financial and campaign features. According to the framework presented in Introduction to Statistical Learning (ISLR), the fundamental goal is to estimate the unknown function, f(x) that relates the predictors, X to the response, Y. Some models are highly flexible and can capture complex non-linear relationships between predictors and the response. In most Cases these models achieve higher predictive accuracy. However, these flexible models often suffer from lower interpretability, making it difficult to understand the influence of individual features. Simpler models such as logistic regression are less flexible but provide coefficients that can be directly interpreted in terms of odds ratios, offering insight into feature importance. 

Chapter 5 of Introduction to Statistical Learning emphasizes the importance of obtaining an unbiased estimate of a model’s test error to assess its predictive performance on unseen data. Test error quantifies how well a model generalizes beyond the data used for training, which is critical for selecting among competing models. The chapter presents several methods for estimating test error, including the simple training/test split, K-fold cross validation, and the leave-one-out cross validation (LOOCV) approach. Each method balances bias and variance differently, with cross validation generally providing a lower-variance estimate compared to a single split. These techniques are essential for model selection, tuning hyperparameters, and comparing the expected performance of alternative models. For the Bank Marketing dataset, estimating test error accurately is particularly important due to the class imbalance and the presence of both categorical and numeric predictors, which can influence model generalization.

In the following chpater of Introduction to Statistical Learning focuses on model selection and regularization techniques to improve predictive performance and reduce overfitting. It introduces methods such as subset selection, shrinkage (Ridge and LASSO regression), and dimension reduction to assess and enhance model accuracy while controlling variance. These approaches allow practitioners to identify parsimonious models that generalize well to new data, providing complementary strategies to traditional test error estimation. We apply these concepts to the models we fit on the bank data, to reduce overfitting and ultimately improve the generalization of our models to new clients. \

We start with a logistic regression (described in methodology section), then build up adjustments and regularizations to the logestic regression to make our baseline. We then move to another interpretable model: KNN. This model is considered transperent in a unique, due to it's neighboring factor.
Finally we fit a Boosting model. This state-of-the-art model is not considered interpretable. Although, there are methods used to extract the contributing variables in such models. We included this model is used to gage how much accucracy we are sacrificing for interpretability. 

The authors employed time ordered data splitting rolling window evaluation. For feature enrichment, they implimented a semi-automatic approach including external intuitive knowledge from domain experts (bank manager) to optimize features. They compared Logistic regression with Decission trees, SVMs, and Neural networks; all complex 'black-box' models. While Moro et al. prioritized maximizing classification performance, our approach also examines model simplicity, automatic feature selection, and diagnostic checks for appropriate classifications (from confusion matrices).


## Methodology
For each of our models we implement a k-fold CV approach, with k=5 training/test split to maintain the distribution of the response variable across the sets. Our models do not apply time-series modeling, since our data does not include sequintial variables. Although variables such as campaign month or day are available within the dataset, which can be considered to hold temporal patterns, we do not model this data as a time series. This keeps the focus on cross-sectional relationships between client characteristics and subscription behavior.

1. **Logistic regression**

```{r, include=FALSE}
full_formula <- y ~ age + job + marital + education + default + balance + 
  housing + loan + contact + day + month + duration + campaign + 
  pdays + previous + poutcome + previously_contacted

glm.full <- glm(full_formula, data = train_data, family = "binomial")

# Perform backward selection
glm.step <- step(glm.full, direction = "backward", trace = 0)


selected_formula <- formula(glm.step)


## 
glm.baseline <- glm(selected_formula, data = train_data, family = "binomial")
summary(glm.baseline)

ctrl <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE, 
  summaryFunction = twoClassSummary 
)

set.seed(445) 
cv_fit <- train(
  selected_formula, 
  data = train_data, 
  method = "glm", 
  family = "binomial",
  trControl = ctrl, 
  metric = "ROC" 
)

test_probabilities <- predict(glm.baseline, newdata = test_data, type = "response")
test_predictions <- factor(ifelse(test_probabilities > 0.5, "yes", "no"), levels = levels(test_data$y))

conf_matrix <- confusionMatrix(data = test_predictions, reference = test_data$y, positive = "yes" )



# Extract deviance residuals
residuals <- residuals(glm.baseline, type = "deviance")
train_data$residuals <- residuals

p1 <- ggplot(train_data, aes(x = balance, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "blue") +
  labs(title = "Deviance Residuals vs. Balance",
       y = "Deviance Residuals") +
  theme_minimal()

glm.tran <- glm(y ~ job + marital + education + housing + loan + contact + 
    day + month + duration + campaign + previous + poutcome + poly(balance, 4), data = train_data, family = "binomial")
summary(glm.tran)
tran.coef <- summary(glm.tran)$coef

residuals <- residuals(glm.tran, type = "deviance")
train_data$residuals <- residuals

p2 <- ggplot(train_data, aes(x = balance, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "blue") +
  labs(title = "Deviance Residuals vs. Balance",
       y = "Deviance Residuals") +
  theme_minimal()


## Regularization
X_full <- model.matrix(y ~ . - 1, data = bank_full)
Y_full <- as.numeric(bank_full$y) - 1 

X_train <- X_full[train_index, ]
Y_train <- Y_full[train_index] 

X_test <- X_full[-train_index, ]
Y_test <- Y_full[-train_index] 


cv.ridge <- cv.glmnet(X_train, Y_train, 
                      alpha = 0, 
                      family = "binomial", 
                      type.measure = "class")
lambda.min.ridge <- cv.ridge$lambda.min

cv.lasso <- cv.glmnet(X_train, Y_train, 
                      alpha = 1, 
                      family = "binomial", 
                      type.measure = "class")
lambda.min.lasso <- cv.lasso$lambda.min

baseline_prob <- predict(glm.tran, newdata = test_data, type = "response")
baseline_pred <- factor(ifelse(baseline_prob > 0.5, "yes", "no"), levels = c("no", "yes"))

ridge_prob <- predict(cv.ridge, s = lambda.min.ridge, newx = X_test, type = "response")
ridge_pred <- factor(ifelse(ridge_prob > 0.5, "yes", "no"), levels = c("no", "yes"))

lasso_prob <- predict(cv.lasso, s = lambda.min.lasso, newx = X_test, type = "response")
lasso_pred <- factor(ifelse(lasso_prob > 0.5, "yes", "no"), levels = c("no", "yes"))


cm_baseline <- confusionMatrix(baseline_pred, test_data$y, positive = "yes")
cm_ridge <- confusionMatrix(ridge_pred, test_data$y, positive = "yes")
cm_lasso <- confusionMatrix(lasso_pred, test_data$y, positive = "yes")

metrics <- data.frame(
  Model = c("Baseline Logit", "Ridge Logit", "LASSO Logit"),
  Accuracy = c(cm_baseline$overall['Accuracy'], cm_ridge$overall['Accuracy'], cm_lasso$overall['Accuracy']),
  Sensitivity = c(cm_baseline$byClass['Sensitivity'], cm_ridge$byClass['Sensitivity'], cm_lasso$byClass['Sensitivity']),
  Specificity = c(cm_baseline$byClass['Specificity'], cm_ridge$byClass['Specificity'], cm_lasso$byClass['Specificity'])
)


roc_baseline <- roc(test_data$y, baseline_prob)
roc_ridge <- roc(test_data$y, as.numeric(ridge_prob))
roc_lasso <- roc(test_data$y, as.numeric(lasso_prob))

roc_data <- data.frame(
  fpr = c(1 - roc_baseline$specificities, 1 - roc_ridge$specificities, 1 - roc_lasso$specificities),
  tpr = c(roc_baseline$sensitivities, roc_ridge$sensitivities, roc_lasso$sensitivities),
  Model = factor(c(rep("Baseline Logit", length(roc_baseline$specificities)),
                   rep("Ridge Logit", length(roc_ridge$specificities)),
                   rep("LASSO Logit", length(roc_lasso$specificities))),
                 levels = c("Baseline Logit", "Ridge Logit", "LASSO Logit"))
)

```

Logistic regression is used to model the probability of a binary outcome as a function of predictor variables. The model estimates the probability that a client subscribes to a term deposit, $P(Y=1 \mid X)$. This model fits with assumptions of independent observations and normality.

Formula: $$f(x) = P(Y=1 \mid \mathbf{x}) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \dots + \beta_k x_k)}}$$  
Coefficients are estimated using the maximum likelihood method, which finds the parameter values that maximize the likelihood of observing the training data (ISLR). 

Feature selection is performed using backward stepwise selection to identify a parsimonious subset of predictors that minimizes estimated test error as measured by AIC. Variables such as duration and previous marketing outcomes remain in the model due to their strong predictive power, whereas less informative variables are excluded to reduce variance. Categorical predictors are converted to factors, and a new binary variable previously_contacted is created to handle clients with no prior campaign contact. This preprocessing ensures that the data is suitable for statistical modeling and facilitates accurate interpretation of model coefficients. We resulted 12 features:

```{r}
selected_formula
```

Cross validation is used to select optimal tuning parameters for these regularized models and to estimate the expected test error. Confusion matrices, sensitivity, specificity, and ROC/AUC metrics are computed to assess predictive performance, especially given the class imbalance where non-subscribers are the majority. This model performed will with mean 5-fold CV test error metrics:

```{r, echo=FALSE}
print(conf_matrix)
```

Next, we check for potential non-linearity in continuous predictors from our predictors remaining after back-ward stepwise selection. We plotted deviance residuals against numeric features.
Based on residual diagnostics, we incorporated a polynomial term (using the poly function: *poly(balance, 4)*) for balance to account for non-linear effects. The Balance squared and power 4 term produced significant coefficients (Last 4 on the list of coefficients).

```{r, out.width="45%", echo=FALSE}
p1
p2
```

```{r}
tran.coef
```

Although this polynomial transformation didn't provide desired metrics in expense of the nonlinear effect. From the deviance residual plots we can observe there weren't significant deviance from a normal residual line. We will not include this model in our comparison.


Regularization techniques such as Ridge ($\ell_2$ penalty) and LASSO ($\ell_1$ penalty) are employed to reduce variance and perform feature selection, particularly in the presence of correlated predictors. LASSO (Least Absolute Shrinkage and Selection Operator) regression uses an $\ell_1$ penalty ($\alpha=1$). A key property of the LASSO is that it forces the coefficients of some variables to be exactly zero, thereby performing automatic feature selection (Chapter 6). The LASSO plot similarly uses cross-validation to select the optimal $\lambda_{\min}$, `r round(lambda.min.lasso, 5)`. Unlike Ridge, the LASSO is expected to yield a more parsimonious model by excluding irrelevant predictors entirely. 

```{r, out.width="45%", echo=FALSE}
plot(cv.ridge)
plot(cv.lasso)
```

Here is a comparison of the models fitted thus far:

```{r}
metrics
```

Ridge and LASSO models yield similar results, indicating that additional shrinkage does not meaningfully improve classification of subscribers. We assume this is due to our modeling being majority indicator variables. This binary aspect of the indicator variable causes it's coefficient to be less affected by regularization techniques.

```{r, out.width="45%", echo=FALSE}
ggplot(roc_data, aes(x = fpr, y = tpr, color = Model)) +
  geom_line(linewidth = 1) +
  geom_abline(linetype = "dashed", color = "grey") +
  labs(
    title = "ROC Curve Comparison for Logistic Models",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  theme_minimal()
```

All three models exhibit a high overall Accuracy (around 0.89), which is expected given the significant class imbalance (Prevalence $\approx 0.11$). However, the critical metric, Sensitivity (the True Positive Rate for the minority class, 'yes'), is very low (around 0.17). This result, consistent across all three models, indicates that they struggle to correctly identify subscribers, leading to many False Negatives (FN). Conversely, the high Specificity (True Negative Rate $\approx 0.98$) means the models are excellent at identifying non-subscribers. The regularization methods (Ridge and LASSO) failed to provide any significant improvement in these metrics, suggesting that the primary limitation is one of high bias (due to insufficient flexibility) rather than high variance.
\

### Models Considered:

We evaluated five main models from the course: ***#only 3 models***


- K-Nearest Neighbors (KNN) — A nonparametric method that makes predictions by finding the k most similar clients. It captures nonlinear relationships but can be sensitive to scaling and high-dimensional data. #TT

K-Nearest Neighbors is a nonparametric classification method that predicts an outcome for a new observation by looking at the k most similar clients in the training dataset. Rather than estimating a set of coefficients or assuming any functional form between predictors and the response, KNN makes predictions based purely on proximity in predictor space. This makes the method particularly well-suited when the underlying relationship between predictors and subscription behavior is nonlinear or too complicated for a parametric model to capture.

Because KNN relies on distance calculations, feature scaling is an important preprocessing step. Otherwise, predictors that are measured on a larger scale-e.g., account balance or call duration-would dominate the distance metric and distort which observations are considered "nearest." In our implementation, we standardized all numeric variables so that each feature was given equal weight in determining the similarity between any two customers. This preprocessing step is especially important for marketing datasets which often involve heterogeneous variables.
A major modeling decision in KNN, which involves a choice, is the value of k. For small values of k, the classifier is highly flexible and closely follows the training data with a potential risk of overfitting noise. Larger values produce smoother, more stable decision boundaries; however, this has a potential risk of missing the important patterns. We used 5-fold cross-validation to identify the value of k that minimized the estimated test error, balancing the bias-variance trade-off.

While KNN presents an intuitive method of finding clients with similar profiles, it has several limitations in this context. The performance may suffer for high-dimensional data, with points becoming further apart and distances becoming less informative-the "curse of dimensionality". Second, KNN is less interpretable than models like logistic regression, where the effect of every predictor can be expressed quantitatively. It does, however, provide a useful baseline when assessing the performance of alternative nonlinear classification methods for predicting term deposit subscriptions.

Boosting was used as the most flexible modeling approach in this study to address the strong class imbalance and complex predictor relationships present in the Bank Marketing dataset. Unlike logistic regression and KNN, boosting does not rely on a fixed functional form and can naturally capture nonlinear effects and interactions among demographic, financial, and campaign-related variables.

The boosting model was trained on the same 80% training set as the other models using a Bernoulli loss function appropriate for binary classification. To allow the model sufficient capacity to learn complex structure, the model was initially trained with 1,500 trees. Model complexity was controlled using a small learning rate and shallow tree depth to encourage gradual improvement rather than abrupt overfitting.
The optimal number of trees was selected using out-of-bag (OOB) error estimation. This procedure identified approximately 1,100 trees as the point at which predictive performance was maximized. Beyond this value, additional trees did not lead to meaningful improvements, indicating that the model had effectively converged. Final predictions and evaluation metrics were therefore computed using 1,100 trees to balance flexibility and generalization.

When evaluated on the test set, boosting achieved the strongest overall performance among all models considered. It produced the highest accuracy, sensitivity, and AUC, substantially improving the identification of the minority “yes” class compared to logistic regression and KNN. In particular, boosting correctly identified over 40% of subscribing clients while maintaining high specificity, demonstrating its ability to reduce false negatives without substantially increasing false positives. These results indicate that boosting is well suited for imbalanced marketing data where identifying potential subscribers is a primary objective.

### Model Training, Formulation, and Validation
All models were trained and evaluated within a consistent validation framework to ensure fair comparison and reliable estimation of out-of-sample performance. An 80/20 stratified train–test split was used, and all modeling decisions, including feature selection and hyperparameter tuning, were made using only the training data. This prevented information leakage and ensured that the test set provided an honest evaluation of generalization performance.

For the logistic regression models, feature selection was performed using backward stepwise selection based on AIC to obtain a parsimonious set of predictors. Residual diagnostics were used to assess model assumptions, leading to the inclusion of nonlinear terms where appropriate. Regularization techniques, including Ridge and LASSO regression, were applied to address multicollinearity and reduce variance. The regularization parameter was selected using 5-fold cross-validation.

K-Nearest Neighbors required additional preprocessing due to its reliance on distance-based calculations. All numeric predictors were standardized to ensure equal contribution to the distance metric. The number of neighbors, 
k, was selected using 5-fold cross-validation by identifying the value that minimized classification error on the training folds.

Boosting was trained using a large number of trees to allow sufficient model flexibility, with the optimal number of trees selected using out-of-bag error estimation. This tuning procedure served a similar role to cross-validation by identifying the point at which additional complexity no longer improved performance.
Across all models, final performance was assessed on the held-out test set using accuracy, sensitivity, and AUC. Because the dataset is highly imbalanced, particular emphasis was placed on sensitivity and AUC as measures of a model’s ability to identify the minority “yes” class.

## Results

This study evaluated multiple statistical and machine learning models for predicting term deposit subscriptions using the Bank Marketing dataset. Due to the strong class imbalance, model performance was assessed using accuracy, sensitivity, and AUC rather than accuracy alone. Logistic regression and its regularized variants demonstrated strong overall discrimination and high AUC values, but they struggled to identify the minority class of subscribing clients. K-Nearest Neighbors performed less effectively, showing limited discriminatory power in this high-dimensional, imbalanced setting.

Boosting outperformed all other models, achieving the highest accuracy, sensitivity, and AUC on the test set. Its superior performance indicates that flexible ensemble methods are better suited for capturing complex, nonlinear relationships in marketing data. However, logistic regression remains valuable due to its interpretability and ability to provide actionable insights. Overall, the results highlight an important trade-off between predictive performance and interpretability when modeling imbalanced marketing data.

## Discussion

The results of this study highlight an important trade-off between predictive performance and interpretability when modeling imbalanced bank marketing data. Among all models evaluated, boosting achieved the strongest overall predictive performance, particularly in terms of sensitivity and AUC. This indicates that boosting is the most effective method for identifying clients who are likely to subscribe to a term deposit, which is the primary objective of the bank. By capturing complex nonlinear relationships and interactions among predictors, boosting substantially reduced the number of missed potential subscribers compared to simpler models.

However, while boosting offers superior predictive accuracy, it lacks transparency. In contrast, regularized logistic regression provides more interpretable results that can be directly translated into business insights. Coefficient estimates from the logistic models reveal how specific client characteristics influence subscription probability. For example, higher account balances were associated with lower marginal increases in subscription probability at extreme values, housing loan status was negatively associated with subscription, and student job status showed a strong positive association. These interpretable effects allow marketing teams to better understand customer behavior and design targeted strategies, even if the model itself is not the most accurate.

Interpretability is particularly important in the banking sector, where predictive decisions may be subject to regulatory oversight and ethical scrutiny. Models that can clearly justify why a client was targeted or excluded are easier to audit and defend. From this perspective, logistic regression remains a valuable tool despite its lower sensitivity. A practical strategy for the bank may involve using boosting as a primary screening model to identify high-potential clients, followed by logistic regression to provide explanations and support decision-making.
Several limitations should be noted. The analysis did not incorporate time-aware modeling, even though campaign timing may influence client responses. Additionally, no resampling techniques were used to directly address class imbalance. Future work could explore methods such as SMOTE or cost-sensitive learning to further improve minority-class detection. Other extensions may include experimenting with alternative ensemble methods or adjusting decision thresholds to better align predictions with business costs.

Overall, this study demonstrates that flexible ensemble models offer substantial gains in predictive performance for imbalanced marketing problems, while interpretable models remain essential for transparency and actionable insight. Balancing these two objectives is critical for effective and responsible deployment of predictive models in real-world banking applications.


## References

Dua, D., & Graff, C. (2019). UCI Machine Learning Repository: Bank Marketing Dataset. 
https://archive.ics.uci.edu/dataset/222/bank+marketing
