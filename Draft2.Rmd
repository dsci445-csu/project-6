---
title: "Draft2"
output: pdf_document
---

## Motivation
- Bank marketing dynamics (cite the paper) 

Bank telemarketing campaigns are a major channel through which financial institutions promote term deposit products. Thsee marketing campaigns come with a cost and are often inefficient due to low client response rates. 
Prior work by Moro et al. (2014) discover that client characteristics and campaign related factors strongly influence subscription outcomes. Motivating a data-driven approach to improve targeting and decision-making in bank marketing. 
    In their approach a Portuguese retail bank was studied using campaign data collected between 2008 and 2013. \
    
- Introduce dataset
    - Description of variables (under factors(
    - Description of target variable (EDA of several factors with target; plots) 
    - note imbalance 
    - Include wrangling of dataset (as.factor() and addition of variable for pdays) 
    
Our dataset was a publicily available daaset, thus had less variables for privacy concerns of the bank customers...
The dataset consists of sixteen variables describing demographic, financial and campaign related variables that enlist individual clients contacted during direct marketing efforts. A list of the 16 variable found below...
The response variable indicates whether a client subscribed to a term deposit which defines a binary classification task. 

Exploratory analysis shows that the numeric predictors (balance, duration, pdays, previously contacted) are heavily skewed.
Also, analysis reveals substantial class imbalance, in the response variable, which motivates the use of appropriate modeling and evaluation for our classification problem.
Visualizations of subscription rates by job education marital status contact type and previous outcome reveal variations in client responsiveness across different factors.

Data preprocessing involved converting categorical variables to factors and adding derived variables such as pdays to account for clients not previously contacted. Converting the categorical variables (initilay character type) to factors creates an indicator for each class in the variable (i.e. if job has 20 factors, 20 indicator variables will be considered in the model fit). As for the previously contaced variable (days since previously contacted), customers who haven't been prreviously contacted will recieve a value of -1. This can cause improper modeling of customers, since the variable now takes on the value of negative numbers. Instead we set -1 to zeros and created a new variable to indicate customers that have not been previously coontacted before. These steps ensure that the dataset is suitable for statistical learning models and enable accurate assessment of predictive performance. \

- Problem Statement
    - introduction of ML (as in ISLR)
        - Chapter 2: Estimating f(x)
            - How some models are more flexible, or more accurate. How some are less interpretable 
            - Assessing model accuracy
        - Chapter 4: We are dealing with classification problem
        
This study is motivated by the availability of bank marketing datasets that enable comparison of machine learning methods for predicting term deposit subscription behavior.

The specification of the problem begins with the recognition that we are addressing a binary classification task where the goal is to predict whether a client will subscribe to a term deposit based on demographic financial and campaign features. According to the framework presented in Introduction to Statistical Learning (ISLR), the fundamental goal is to estimate the unknown function, f(x) that relates the predictors, X to the response, Y. Some models are highly flexible and can capture complex non-linear relationships between predictors and the response. In most Cases these models achieve higher predictive accuracy. However, these flexible models often suffer from lower interpretability, making it difficult to understand the influence of individual features. Simpler models such as logistic regression are less flexible but provide coefficients that can be directly interpreted in terms of odds ratios, offering insight into feature importance. 

Chapter 5 of Introduction to Statistical Learning emphasizes the importance of obtaining an unbiased estimate of a model’s test error to assess its predictive performance on unseen data. Test error quantifies how well a model generalizes beyond the data used for training, which is critical for selecting among competing models. The chapter presents several methods for estimating test error, including the simple training/test split, K-fold cross validation, and the leave-one-out cross validation (LOOCV) approach. Each method balances bias and variance differently, with cross validation generally providing a lower-variance estimate compared to a single split. These techniques are essential for model selection, tuning hyperparameters, and comparing the expected performance of alternative models. For the Bank Marketing dataset, estimating test error accurately is particularly important due to the class imbalance and the presence of both categorical and numeric predictors, which can influence model generalization.

In the following chpater of Introduction to Statistical Learning focuses on model selection and regularization techniques to improve predictive performance and reduce overfitting. It introduces methods such as subset selection, shrinkage (Ridge and LASSO regression), and dimension reduction to assess and enhance model accuracy while controlling variance. These approaches allow practitioners to identify parsimonious models that generalize well to new data, providing complementary strategies to traditional test error estimation. We apply these concepts to the models we fit on the bank data, to reduce overfitting and ultimately improve the generalizability of our models to new clients. \

    - Models we will implement (without diving into details)
We start with a logistic regression (described in methodology section), then build up adjustments and regularizations to the logestic regression to make our baseline. 
We then move to another interpretable model: KNN. This model is considered transperent in a unique, due to it's neighboring factor.
Finally we fit a Boosting model. This state-of-the-art model is not considered interpretable. Although, there are methods used to extract the contributing variables in such models. We included this model is used to gage how much accucracy we are sacrificing for interpretability. 

    - Bank marketing dynamics (cite the paper) differentiate our methodology
The authors employed time ordered data splitting rolling window evaluation. 
For feature enrichment, they implimented a semi-automatic approach including external intuitive knowledge from domain experts (bank manager) to optimize features. 
They compared Logistic regression with Decission trees, SVMs, and Neural networks; all complex 'black-box' models.
While Moro et al. prioritized maximizing classification performance, our approach also examines model simplicity, automatic feature selection, and diagnostic checks for appropriate classifications (from confusion matrices).

## Methodology
For each of our models we implement a k-fold CV approach, with k=5 training/test split to maintain the distribution of the response variable across the sets.

1. Logistic regression 
    * Describe fitting algorithm (from ISLR)
    * code form model_fir
    * short paragraph after each action
        * “we did this because…” or “we didn’t do this because…”
Logistic regression is used to model the probability of a binary outcome as a function of predictor variables. The model estimates the probability that a client subscribes to a term deposit, $P(Y=1 \mid X)$

Formula: f(x) = 

Coefficients are estimated using the maximum likelihood method, which finds the parameter values that maximize the likelihood of observing the training data.

Feature selection is performed using backward stepwise selection to identify a parsimonious subset of predictors that minimizes estimated test error as measured by AIC. Variables such as duration and previous marketing outcomes remain in the model due to their strong predictive power, whereas less informative variables are excluded to reduce variance. Categorical predictors are converted to factors, and a new binary variable previously_contacted is created to handle clients with no prior campaign contact. This preprocessing ensures that the data is suitable for statistical modeling and facilitates accurate interpretation of model coefficients.

Next, we check for potential non-linearity in continuous predictors from our predictors remaining after back-ward stepwise selection. We plotted deviance residuals against numeric features.
Based on residual diagnostics, we incorporated a polynomial term for balance to account for non-linear effects.

Regularization techniques such as Ridge (l2 penalty) and LASSO (l1 penalty) are employed to reduce variance and perform feature selection, particularly in the presence of correlated predictors. Cross validation is used to select optimal tuning parameters for these regularized models and to estimate the expected test error. Confusion matrices, sensitivity, specificity, and ROC/AUC metrics are computed to assess predictive performance, especially given the class imbalance where non-subscribers are the majority.

Ridge and LASSO models yield similar results, indicating that additional shrinkage does not meaningfully improve classification of subscribers. We assume this is due to our modeling being majority indicator variables. This binary aspect of the indicator variable causes it's coefficient to be less affected by regularization techniques.
The ROC curves and AUC values of approximately 0.90 confirm that all models perform similarly in terms of discrimination between the two classes. These findings suggest that the primary limitation is bias from model flexibility rather than variance, and highlight the need for potentially more flexible classifiers to improve minority class prediction.

  2. KNN
  3. Boasted trees


## Results and Discussion
- Declare we are looking for specificity and interpretable (and cite back to specification of problem from introduction)
- Stats
    - Table of metrics for all models 
    - plot AUC curve 
- Business impact
    - Demonstrate interpretable models benefits
- Future work

Our methodology differentiates from prior work by Moro et al. (2014) in focusing on a comparative evaluation of interpretable models with minimal feature engineering, emphasizing a balance between predictive power and model transparency.

Model performance was primarily assessed using the area under the ROC curve which provides a threshold independent measure of classification quality. In contrast our work focuses on the original bank marketing dataset and emphasizes comparative evaluation of standard statistical learning models with reduced feature complexity and greater interpretability.


## References
1. dataset
2. Paper on website
3. ISLR v2
